\chapter{Fundamentos Teóricos}\label{cap-fundamentos}

\section{Detectores por Produto Interno com Minimização do Erro Quadrático Médio (IPD)} \label{sec_IPD}
               
O projeto do classificador utilizando detector por produto interno pode ser feito da seguinte forma: suponhamos uma variável aleatória, ou seja, uma variável cujo os seus resultados sejam imprevisíveis, $X_{dx1}$. Suas realizações podem ser classificadas nas classes {${A_1},\,{A_2},\,...,\,{A_n}$} ou na classe B, sendo que a classe ${A_i}$ representa os padrões que desejamos encontrar e B representa os demais padrões que não seja de interesse. As probabilidades de encontrarmos essas classes são dadas por:
                 
\begin{equation}\label{eq_prob}
\begin{array}{l}
p({A_i}) = p(\mathcal{X} \in {A_i}) = {p_i}\\
p(B) = p(\mathcal{X} \in B)
\end{array}
\end{equation}

O classificador \textbf{h} é projetado de tal forma que o produto interno dele com uma entrada $\mathcal{X}$ seja dado por:
                
\begin{equation}\label{eq_probabilidades}
<\mathcal{X}, h_{A_{i}}>=h^{t}_{A_{i}}\mathcal{X}=C 
\end{equation}
                
Onde $\mathcal{C}$=1 caso $\mathcal{X} \in A_i$  e $\mathcal{C}=0$ para $\mathcal{X} \in A_{i}$.
                
Se tomarmos o problema dos mínimos quadrados que tenta minimizar a soma dos quadrados das diferenças entre o valor estimado e os dados observados, a equação \eqref{eq_probabilidades} procura encontrar o melhor classificador possível. Sendo assim, podemos definir o erro como:

\begin{equation}
\varepsilon=h^{t}_{Ai}\mathcal{X}-\mathcal{C}
\end{equation}
        
Assim, considerando-se definição de erro quadrático temos:
                
\begin{equation}
||\varepsilon||^{2}=(h^{t}_{Ai}\mathcal{X}-\mathcal{C})(h^{t}_{Ai} \mathcal{X}-\mathcal{C})^{t}\label{eq_erroQuad}
\end{equation}
                
Assumindo que \textit{$h_{A_{i}}$}, $\mathcal{X}$ e $\mathcal{C}$ como valores reais e desenvolvendo  a equação \eqref{eq_erroQuad} chegaremos ao valor esperado do erro quadrático como sendo:
                

\begin{equation} \label{eq_erroQuadratico}
E[||\varepsilon|{|^2}] = h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]
\end{equation}
                
Afim de minimizarmos o erro quadrático da equação \eqref{eq_erroQuadratico} igualamos a zero (0) o seu gradiente em relação a \textit{$h_{A_{i}}$}. Dessa forma, obtemos a seguinte expressão:
                
\[
\begin{array}{l}
\frac{\partial E[||\varepsilon |{|^2}]}{{\partial {h_{Ai}}}} = \frac{\partial }{{\partial {h_{Ai}}}}\{ h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]\} \\
0 = \{ E[\mathcal{X}{\mathcal{X}^t}] + E{[\mathcal{X}{\mathcal{X}^t}]^t}\} {h_{Ai}} - 2E[\mathcal{XC}]\\
2E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2E[\mathcal{XC}] = 0
\end{array}
\]
                
Logo,

\begin{equation}\label{vetorH}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} = E[\mathcal{XC}]\\
{h_{Ai}} = {\{ E[\mathcal{X}{\mathcal{X}^t}]\} ^{ - 1}}E[\mathcal{XC}]
\end{array}
\end{equation}
                
Os termos $E[\mathcal{X}{\mathcal{X}^t}]$ e $E[\mathcal{XC}]$ até então são desconhecidos. Podemos desenvolver o termo $E[\mathcal{X}{\mathcal{X}^t}]$ da seguinte forma:

\begin{equation}\label{eqSeiSete}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})
\end{equation}

Lembrando o iníco da seção onde definimos as classes ${A_1},\,{A_2},\,...,\,{A_n}$ e a classe B onde podemos encontrar ${B_1},\,{B_2},\,...,\,{B_n}$, podemos encontrar as médias da equação \eqref{eqSeiSete} da seguinte forma:

\begin{equation}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})\\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {p({A_j})\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} \\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} 
\end{array}
\end{equation}

Conforme verificamos na equação \eqref{eq_probabilidades}, obteremos a saída igual a 1 para o caso da variável aleatória X for classificada na classe $A_i$ e 0 caso contrário. Seguindo o raciocínio desenvolvido para a classe A temos as classes representadas através do complemento de $A_i$, dado por $A_i^C$:

\begin{equation}
\begin{array}{l}
E[\mathcal{XC}] = E[\mathcal{XC}|{A_i}]p({A_i}) + E[\mathcal{XC}|A_i^C](1 - p({A_i}))\\
 = E[\mathcal{X}|{A_i}]p({A_i}) + 0(1 - p({A_i}))\\
 = {p_i}\frac{1}{{{L_1}}}\sum\limits_{k = 1}^{{L_1}} {{A_{ik}}} 
\end{array}
\end{equation} 

Assim, a equação \eqref{vetorH}, pode ser reescrita da forma:

\begin{equation}
{h_{Ai}} = {\{ p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t}  + \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}\sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} } \} ^{ - 1}}\{ {p_i}\frac{1}{{{L_i}}}\sum\limits_{k = 1}^{{L_i}} {{A_{ik}}} \}
\end{equation}

Considerando-se a matriz de autocorrelação de $B$ igual a $R_B$ e a matriz $A_i$ igual a $R_{Ai}$, e a média $A_i$ igual a $\mu_{Ai}$, podemos expressar os resultados em função das variáveis aleatórias, assim, temos:

\begin{equation}\label{vetorH_final}
{h_{Ai}} = {\{ p(B){R_B} + \sum\limits_{j = 1}^n {{p_j}{R_{Aj}}} \} ^{ - 1}}\{\sum\limits_{i = 1}^m {{p_i}{\mu _{Ai}}}\}
\end{equation}

A condição de existência do classificador projetado é que o primeiro termo da equação \eqref{vetorH_final} seja uma matriz inversível e a dimensão dos vetores de entrada deve ser menor que a soma dos números de elementos de todas as classes $A_j$ e B, ou seja $d \le \,r\, + \,\sum\limits_{j = 1}^n {{L_j}}$.
              
\section{Detector por Produto Interno utilizando Análise de Componentes Principais (IPD-B-PCA)}

O Detector por Produto Interno (do inglês, \textit{Inner Product Detector} - IPD) nada mais é que um classificador construído a partir de um treinamento de imagens previamente selecionadas. Esses classificadores são considerados rápidos por realizarem a classificação através de uma operação simples de produto interno, além de minimizarem o erro quadrático \cite{waldir}. Com o intuito de aumentar sua robustez a pequenas rotações, Sabino  \cite{waldir} acrescentou a técnica de Análise de Componentes Principais (do inglês, Principal Component Analysis - PCA).

Segundo Sabino \cite{waldir}, a principal motivação para a aplicação da técnica de PCA reside no fato de que os padrões analisados (faces, por exemplo) não são distribuídos de forma aleatória, e mesmo essas estando distribuídas em um espaço de dimensão elevada podem ser representadas por um subespaço de menor dimensão. O conjunto dessas componentes principais são denominadas \textit{eigenfaces} (o termo \textit{eigenfaces} deve-se ao fato da representação das faces através dos autovetores do conjunto de faces).

\subsection*{\textit{Eigenfaces} e \textit{Eigenpoints}}

O termo \textit{eigenfaces} refere-se a um algoritmo que faz uso do padrão comum existente nas faces para representá-los em um subespaço de dimensão inferior ao original, ou seja, a partir de diversas faces com pequenas inclinações podemos gerar um único "template"\ que corresponde a representação de todos. Quando aliamos essa técnica ao IPD conseguimos um classificador mais robusto a pequenas rotações.

Matematicamente podemos obter as \textit{eigenfaces} como sendo \textit{M} realizações de uma variável aleatória $\mathcal{X}_{Nx1}$ iguais a $x_{\textit{i}}$. Cada coluna $x_{\textit{i}}$ contém os \textit{pixels} de uma imagem da face ${\gamma_\textit{i}}$, onde \textit{i} = \{1,..., M\}. Dessa forma, \textbf{X} é uma matriz formada por \textit{M} vetores (sendo \textit{M} > \textit{N}) da seguinte forma:

\begin{equation}
X = [{x_1} - {\mu _\mathcal{X}},\,...,\,{x_M} - {\mu _\mathcal{X}}]
\end{equation}

Onde $\mu _\mathcal{X}$ é:

\begin{equation}
{\mu _\mathcal{X}} = \frac{1}{M}\sum\limits_{i = 1}^M {{x_\textit{i}}}
\end{equation}

A matriz de covariância de $\mathcal{X}$, $\sum\nolimits_\mathcal{X}$ é dada por:

\begin{equation}
\sum\nolimits_\mathcal{X}  =  \frac{1}{{M - 1}}X{X^T}
\end{equation}

Dessa forma, a base que contém as \textit{N} componentes principais descorrelacionadas (\textit{eingenfaces}) é encontrada através da diagonalização da matriz $\sum\nolimits_\mathcal{X}$, assim:

\begin{equation}
\Lambda  = {\Phi ^T}\sum\nolimits_\mathcal{X} \Phi
\end{equation}

onde $\Phi  = [{\phi _1},\,{\phi _2},\,...,\,{\phi _N}]$ é a matriz de autovetores de $\sum\nolimits_\mathcal{X}$, o sobrescrito \textit{T} indica o transposto da matriz de covariância e $\Lambda$ é a matriz diagonal contendo os valores de $\sum\nolimits_\mathcal{X}$, onde cada autovalor ${\lambda _\textit{i}}$ tem uma variância maior ou igual ao seu sucessor, ou seja, ${\lambda _1} \ge {\lambda _2} \ge ... \ge {\lambda _N}$. Dessa forma é possível selecionar os "melhores"\ componentes através dos autovalores ${\lambda _i}$. Assim, podemos dizer que o método PCA e os \textit{eingenfaces} são as \textit{k} componentes principais de $\Phi_{\textit{i}}$.

Nesse contexto podemos utilizar esse raciocínio para ao invés de faces tratar características salientes comum as faces (canto da boca, olho, nariz, etc), a esses atributos damos o nome de pontos fiduciais (do inglês, \textit{fiducial point}). Quando aplicamos a teoria de \textit{eigenfaces} aos pontos fiduciais obteremos os \textit{eigenpoints}. O procedimento matemático para obtermos os \textit{eigenpoints} é o mesmo dos \textit{eigenfaces}, bastando somente delimitar os \textit{pixels} em blocos centrados no ponto fiducial de interesse. As Figuras \ref{fig:eigenfacesEx} e \ref{fig:eigenpointsEx} ilustram os dois métodos.

\begin{figure}[h]
\subfigure[Exemplos de \textit{eigenfaces}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenfacesEx.png}
\label{fig:eigenfacesEx}
}
\subfigure[Exemplos de \textit{eigenpoints}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenPointsEx.png}
\label{fig:eigenpointsEx}}
\caption{Exemplos de \textit{eigenfaces}  à direita e a esquerda temos \textit{eigenpoints} extraídos a partir de blocos centrados a partir do olho direito das imagens das faces.}
\end{figure}

\subsection*{Formulação do IPD-B-PCA}

O método proposto por Sabino \cite{waldir} consiste em aumentar a robustez dos filtros discriminativos através das componentes principais de maior energia, ou seja, cujos autovalores associados são maiores. Matematicamente podemos obter esses filtros da seguinte maneira: suponhamos uma  variável aleatória que pode ser classificadas nas classes \{${\mathcal{A}_1},\,{\mathcal{A}_2},\,...,\,{\mathcal{A}_N}$\} onde: 

\begin{itemize}
\item A classe ${\mathcal{A}_1}$ será composta pela componente que desejamos detectar (${\mathcal{A}_1=\{\phi _i\}}$);

\item A classe ${\mathcal{A}_2}$ refere-se a deslocamentos lineares de $\phi _i$. A expressão $\phi _i^{n,m}$ relaciona o deslocamento linear da componente \textit{i} em \textit{n} \textit{pixels} na direção horizontal e \textit{m} na vertical. Considerando-se blocos quadrados de tamanho \textit{L} em que o centro do bloco contém o ponto fiducial de interesse, podemos escrever ${\mathcal{A}_2} = \{ \phi _i^{n,m}$, onde: $- L \le \,m,n\, \le \,L$, com $n,m \ne \,0\}$ .

\item A classe $\mathcal{B}$ será composta pelas demais componentes $\phi _j$, ou seja, as classes \{ ${\mathcal{A}_3},\,{\mathcal{A}_4},\,...,\,{\mathcal{A}_N}$\}.
\end{itemize}

Para projetarmos o nosso classificador utilizaremos a equação \eqref{vetorH_final}, levando-se em consideração as premissas citadas teremos:

\begin{equation}\label{eq_ipd_pca}
{h_{{\phi _1}}} = {\left\{ {{p_1}{R_{A1}} + {p_2}{R_{A2}} + {p_B}{R_B}} \right\}^{ - 1}}\,{p_1}{R_{A1}}
\end{equation}

A Figura \ref{fig_ipd_pca} ilustra o esquema proposto, onde temos \textit{S} \textit{eigenpoints} ($\Phi  = \,{\phi _1},\,{\phi _2},\,...,\,{\phi _S}$), para cada um será projetado um filtro, de forma que estejam disposto do maior para o menor valor de variância (autovalores), desse modo, ${\lambda _1} > \,{\lambda _2} > ,...,\,{\lambda _S}$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.9]{figuras/esquema_Matriz.png}
\caption{\textit{S} classificadores para \textit{S} componentes principais.}
\label{fig_ipd_pca}
\end{figure}

Na Figura \ref{fig_schema}, podemos observar um \textit{eigenpoint} que refere-se ao olho direito considerando-se apenas a primeira componente principal. A componente $\phi_1$ foi calculada através de blocos de dimensão 2L+1 (imagem mais à esquerda). Afim de obtermos a componente $\phi_2$ através de deslocamentos lineares até o meio do bloco calculamos $\mathop {{\phi _i}}\limits^ \sim$ para blocos de dimensão 4L+1 (imagem mais a direita). Assim, reescrevendo a Equação \eqref{eq_ipd_pca} em função dessas ponderações teremos:

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{figuras/Schema.png}
\caption{Desclocamento linear}
\label{fig_schema}
\end{figure}


\begin{equation} \label{eq_IPD-B-PCA_Final}
h{\phi _i} = {\left\{ \begin{array}{l}
{p_1}{\phi _i}{({\phi _i})^{T}} + {p_2}\frac{1}{{{{(2L + 1)}^2} - 1}}\,\sum\limits_{n =  - L\hfill\atop
n \ne 0\hfill}^L {\sum\limits_{m =  - L\hfill\atop
m \ne 0\hfill}^L {{\phi _i}^{(n,m)}({\phi _i}^{(n,m)})^{T} }} +\\
 + p(\mathcal{B})\,\frac{1}{{N - 1}}\,\sum\limits_{j = 1\hfill\atop
j \ne i\hfill}^N {{\phi _j}{{({\phi _j})}^{T}}} 
\end{array} \right\}^{ - 1}}\,{p_1}{\phi _i}
\end{equation}

Finalmente, a Equação \eqref{eq_IPD-B-PCA_Final} expressa o detector por produto interno utilizando análise de componentes principais. É interessante destacarmos que na construção dos padrões de interesse da classe $\mathcal{A}_1$ podemos utilizar componentes levemente rotacionados ou deslocados, dessa forma é possível construir detectores mais robustos. A classe ${\mathcal{A}_2}$ é de fundamental importância, uma vez que se considerássemos somente as classes padrões ($\mathcal{A}_1$) poderíamos incorrer em um possível erro e detectar algo próximo ao padrão mesmo não sendo. Esse fato, deve-se a linearidade dos filtros, portanto, pequenas alterações na entrada produzem apenas pequenas perturbações na saída. Assim, a adição da classe ${\mathcal{A}_2}$ permite ao classificador rejeitar pequenos deslocamentos lineares de forma mais precisa e com uma maior eficiência.
                      
\section{Computação Paralela}

Com o desenvolvimento da informática em diversos campos houve uma exigência por um maior poder de processamento por parte dos programas. \textit{Softwares} tornaram-se mais sofisticados e computacionalmente mais pesados, assim os processadores começaram a ficar obsoletos face à essa excesso de demanda por processamento. A solução encontrada no início foi acelerar o relógio do processador, porém, com essa aceleração apareceu o problema do superaquecimento desses chips. A solução para esse problema veio com a adição de mais de um núcleo no mesmo \textit{chip}, através da tecnologia \textit{multicore}. Dessa forma, eliminaria-se a necessidade de cada núcleo trabalhar em frequências muito elevadas, tendo somente como base um esquema de escalonamento de tarefas mais eficiente \cite{tanenbaum2013sistemas}.

A ideia desse paradigma de programação é "quebrar" o programa em várias partes e processá-los de forma concorrente (paralela), ganhando assim tempo na execução da tarefa. É interessante notarmos que nem todas as partes de um programa são "paralelizáveis", portanto para esse tipo de aplicação a existência de um ou mais núcleos não resultará na celeridade de sua execução. O paralelismo pode ser explícito quando o próprio programador controla a concorrência ou implícito, quando cabe ao compilador do sistema realizar o paralelismo.

Existem diversos tipos de paralelismo, podermos destacar: 

\begin{itemize}
\item \textbf{No bit}: Nesse tipo dobrava-se o tamanho da palavra (unidade de informação utilizada para cada tipo de computador), com isso dobrava-se a quantidade de processamento por ciclo e consequentemente a diminuição da quantidade de instruções \cite{sriram2009embedded};

\item \textbf{Na instrução}: Assumindo que um programa de computador é constituído por um conjunto de instruções interpretados pelo processador, pode-se combinar essas instruções em blocos que podem ser executados de forma concorrente sem alterar o resultado final\cite{sriram2009embedded};

\item \textbf{No dado}: Geralmente ocorre em laços (\textit{loops}), nas quais uma interação depende de outros resultados\cite{sriram2009embedded};

\item \textbf{No tarefa}: É inerente as aplicações (programas), caracteriza-se ao oposto do paralelismo em dados, ou seja, diferentes cálculos podem ser realizados no mesmo ou em diferentes tipos de dados\cite{sriram2009embedded};

Um ponto crucial para se obter um alto desempenho na computação paralela tange no que diz respeito ao acesso da memória. Existem duas arquiteturas quanto as formas de acesso a memória:

\item \textbf{Centralized Multiprocessor ou Uniform Memory Access Multiprocessor (UMA)}: Nesse modelo as tarefas compartilham de um mesmo espaço de memória e a comunicação é mais simples, pois utiliza-se de uma área comum na memória (Figura \ref{fig_Compartilhada} a);

\item \textbf{Distributed Multiprocessor ou Nonuniform Memory Access Multiprocessor (NUMA)}: Nesse padrão cada processador possui o seu próprio endereço na memória. A comunicação entre esses processadores é feito através de mensagens (Figura \ref{fig_Ditribuida} b). {\color{red} Eu vou concatenar as duas Figuras em uma só. Só não fiz porque ainda não acertei como se faz})
\end{itemize}

\begin{figure}[h]
\center
\subfigure[Memórica compartilhada - UMA]{\includegraphics[width=7cm]{figuras/Tipos_Memorias_Shared.PNG}}\label{fig_Compartilhada}
\qquad
\subfigure[Memórica compartilhada - NUMA]{\includegraphics[width=7cm]{figuras/Tipos_Memorias.PNG}}
\caption{Arquiteturas de computação paralela a) Memória compartilhada e b) memória distribuída.}\label{fig_Ditribuida}
\end{figure}


\subsection{OpenMP}

Quanto ao paralelismo, podemos destacar duas formas: o paralelismo explícito e o paralelismo implícito. A seguir destacamos algumas características de cada uma:

\begin{itemize}
\item \textbf{paralelismo explícito}:

\item \textbf{paralelismo implícito}:

\end{itemize}


\subsection{Métricas de desempenho na Computação Paralela}

{\color{red} A idéia aqui aqui é apresentar o/os métodos mai utilizados na avaliação de desempenho quando comparados com a programação sequencial.}





