\chapter{Fundamentos Teóricos}\label{cap-fundamentos}

\section{Detectores por Produto Interno com Minimização do Erro Quadrático Médio (IPD)} \label{sec_IPD}
               
O projeto do classificador utilizando detector por produto interno pode ser feito da seguinte forma: suponhamos uma variável aleatória, ou seja, uma variável cujo os seus resultados sejam imprevisíveis, $X_{dx1}$. Suas realizações podem ser classificadas nas classes {${A_1},\,{A_2},\,...,\,{A_n}$} ou na classe B, sendo que a classe ${A_i}$ representa os padrões que desejamos encontrar e B representa os demais padrões que não seja de interesse. As probabilidades de encontrarmos essas classes são dadas por:
                 
\begin{equation}\label{eq_prob}
\begin{array}{l}
p({A_i}) = p(\mathcal{X} \in {A_i}) = {p_i}\\
p(B) = p(\mathcal{X} \in B)
\end{array}
\end{equation}

O classificador \textbf{h} é projetado de tal forma que o produto interno dele com uma entrada $\mathcal{X}$ seja dado por:
                
\begin{equation}\label{eq_probabilidades}
<\mathcal{X}, h_{A_{i}}>=h^{t}_{A_{i}}\mathcal{X}=C 
\end{equation}
                
Onde $\mathcal{C}$=1 caso $\mathcal{X} \in A_i$  e $\mathcal{C}=0$ para $\mathcal{X} \in A_{i}$.
                
Se tomarmos o problema dos mínimos quadrados que tenta minimizar a soma dos quadrados das diferenças entre o valor estimado e os dados observados, a equação \eqref{eq_probabilidades} procura encontrar o melhor classificador possível. Sendo assim, podemos definir o erro como:

\begin{equation}
\varepsilon=h^{t}_{Ai}\mathcal{X}-\mathcal{C}
\end{equation}
        
Assim, considerando-se definição de erro quadrático temos:
                
\begin{equation}
||\varepsilon||^{2}=(h^{t}_{Ai}\mathcal{X}-\mathcal{C})(h^{t}_{Ai} \mathcal{X}-\mathcal{C})^{t}\label{eq_erroQuad}
\end{equation}
                
Assumindo que \textit{$h_{A_{i}}$}, $\mathcal{X}$ e $\mathcal{C}$ como valores reais e desenvolvendo  a equação \eqref{eq_erroQuad} chegaremos ao valor esperado do erro quadrático como sendo:
                

\begin{equation} \label{eq_erroQuadratico}
E[||\varepsilon|{|^2}] = h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]
\end{equation}
                
Afim de minimizarmos o erro quadrático da equação \eqref{eq_erroQuadratico} igualamos a zero (0) o seu gradiente em relação a \textit{$h_{A_{i}}$}. Dessa forma, obtemos a seguinte expressão:
                
\[
\begin{array}{l}
\frac{\partial E[||\varepsilon |{|^2}]}{{\partial {h_{Ai}}}} = \frac{\partial }{{\partial {h_{Ai}}}}\{ h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]\} \\
0 = \{ E[\mathcal{X}{\mathcal{X}^t}] + E{[\mathcal{X}{\mathcal{X}^t}]^t}\} {h_{Ai}} - 2E[\mathcal{XC}]\\
2E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2E[\mathcal{XC}] = 0
\end{array}
\]
                
Logo,

\begin{equation}\label{vetorH}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} = E[\mathcal{XC}]\\
{h_{Ai}} = {\{ E[\mathcal{X}{\mathcal{X}^t}]\} ^{ - 1}}E[\mathcal{XC}]
\end{array}
\end{equation}
                
Os termos $E[\mathcal{X}{\mathcal{X}^t}]$ e $E[\mathcal{XC}]$ até então são desconhecidos. Podemos desenvolver o termo $E[\mathcal{X}{\mathcal{X}^t}]$ da seguinte forma:

\begin{equation}\label{eqSeiSete}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})
\end{equation}

Lembrando o iníco da seção onde definimos as classes ${A_1},\,{A_2},\,...,\,{A_n}$ e a classe B onde podemos encontrar ${B_1},\,{B_2},\,...,\,{B_n}$, podemos encontrar as médias da equação \eqref{eqSeiSete} da seguinte forma:

\begin{equation}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})\\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {p({A_j})\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} \\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} 
\end{array}
\end{equation}

Conforme verificamos na equação \eqref{eq_probabilidades}, obteremos a saída igual a 1 para o caso da variável aleatória X for classificada na classe $A_i$ e 0 caso contrário. Seguindo o raciocínio desenvolvido para a classe A temos as classes representadas através do complemento de $A_i$, dado por $A_i^C$:

\begin{equation}
\begin{array}{l}
E[\mathcal{XC}] = E[\mathcal{XC}|{A_i}]p({A_i}) + E[\mathcal{XC}|A_i^C](1 - p({A_i}))\\
 = E[\mathcal{X}|{A_i}]p({A_i}) + 0(1 - p({A_i}))\\
 = {p_i}\frac{1}{{{L_1}}}\sum\limits_{k = 1}^{{L_1}} {{A_{ik}}} 
\end{array}
\end{equation} 

Assim, a equação \eqref{vetorH}, pode ser reescrita da forma:

\begin{equation}
{h_{Ai}} = {\{ p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t}  + \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}\sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} } \} ^{ - 1}}\{ {p_i}\frac{1}{{{L_i}}}\sum\limits_{k = 1}^{{L_i}} {{A_{ik}}} \}
\end{equation}

Considerando-se a matriz de autocorrelação de $B$ igual a $R_B$ e a matriz $A_i$ igual a $R_{Ai}$, e a média $A_i$ igual a $\mu_{Ai}$, podemos expressar os resultados em função das variáveis aleatórias, assim, temos:

\begin{equation}\label{vetorH_final}
{h_{Ai}} = {\{ p(B){R_B} + \sum\limits_{j = 1}^n {{p_j}{R_{Aj}}} \} ^{ - 1}}\{\sum\limits_{i = 1}^m {{p_i}{\mu _{Ai}}}\}
\end{equation}

A condição de existência do classificador projetado é que o primeiro termo da equação \eqref{vetorH_final} seja uma matriz inversível e a dimensão dos vetores de entrada deve ser menor que a soma dos números de elementos de todas as classes $A_j$ e B, ou seja $d \le \,r\, + \,\sum\limits_{j = 1}^n {{L_j}}$.
              
\section{Detector por Produto Interno utilizando Análise de Componentes Principais}\label{cap-IPD-PCA}

Com o intuito de aumentar a robustez do detector por produto interno (IPD) a pequenas rotações, utilizou-se a técnica de análise de componentes principais (do inglês, \textit{principal component analysis} - PCA) \cite{waldir}. Essa técnica baseia-se na extração de um número desejado de componentes principais de um vetor multidimensional, de modo que ao final da operação tenhamos um vetor cuja dimensão é inferior à original e sua variância seja maximizada \cite{waldir}. A seguir comentaremos sobre o método denominado \textit{eigenpoint}, que é utilizado para encontrar pontos fiduciais\footnote{do inglês, \textit{fiducial point}, refere-se a características salientes comum as faces, como canto da boca, olho, nariz, etc.}

\subsection*{\textit{Eigenpoints}}

O termo \textit{eigenpoints} refere-se a um método que faz uso de PCA para a detecção de pontos fiduciais em imagens. A análise de componentes principais, é associada à ideia de redução de dimensionalidade,  com  menor  perda  possível  da  informação. Tendo em vista a similaridade dos pontos fiduciais humanos bem como a localização destes, utiliza-se essa técnica para representá-los em um subespaço de dimensão inferior ou igual ao original, assim, podemos utilizar \textit{eigenpoints} para encontrarmos os pontos fiduciais de um conjunto de faces, por exemplo.

 \textit{Eigenpoints} constitui-se um caso específico de \textit{eigenfaces} (Figura \ref{fig:eigenfacesEx}), porém, ao invés de utilizarmos toda a face tomamos um ponto fiducial de interesse e recortamos o bloco em que ele está contido, conforme pode ser visto na Figura \ref{fig:eigenpointsEx} (na Figura \ref{fig:eigenfacesEx} temos o \textit{eigenface} e na Figura \ref{fig:eigenpointsEx} recortamos o bloco no qual está contido o olho direito - \textit{eigenpoint}).

Matematicamente, podemos obter os \textit{eigenpoints} como sendo $M$ realizações de uma variável aleatória $\mathcal{X}_{N \times1}$ iguais a x$_{\textit{i}}$. Cada coluna x$_{\textit{i}}$ contém os \textit{pixels} de blocos centrados em pontos fiduciais ${\gamma_\textit{i}}$, onde \textit{i} = \{1,..., M\}. Dessa forma, \textbf{X} é uma matriz formada por \textit{M} vetores (sendo \textit{M} > \textit{N}) da seguinte forma:

\begin{equation}
\textbf{X} = [{\rm{x}_1} - {\mu _\mathcal{X}},\,...,\,{\rm{x}_M} - {\mu _\mathcal{X}}]
\end{equation}

onde $\mu _\mathcal{X}$ é:

\begin{equation}
{\mu _\mathcal{X}} = \frac{1}{M}\sum\limits_{i = 1}^M {{\rm{x}_\textit{i}}}
\end{equation}

Seja \textbf{$\mathbf{\Sigma}_\mathcal{X}$} a matriz de covariância de $\mathcal{X}$, dada por:

\begin{equation}
\mathbf{\Sigma}_\mathcal{X}  =  \frac{1}{{M - 1}}\textbf{X}{\textbf{X}^{*T}}
\end{equation}

Dessa forma, a base que contém as \textit{N} componentes principais descorrelacionadas (\textit{eingenpoints}) é encontrada através da diagonalização da matriz $\mathbf{\Sigma}_\mathcal{X}$, assim:

\begin{equation}
\mathbf{\Lambda} = \mathbf{\Phi}^{*t} \mathbf{\Sigma}_{\mathcal{X}} \mathbf{\Phi}.
\end{equation}

sendo $\Phi  = [{\phi _1},\,{\phi _2},\,...,\,{\phi _N}]$ é a matriz de autovetores de $\mathbf{\Sigma}_\mathcal{X}$, o sobrescrito \textit{T} indica o transposto da matriz de covariância e $\Lambda$ é a matriz diagonal contendo os valores de $\sum\nolimits_\mathcal{X}$, onde cada autovalor ${\lambda _\textit{i}}$ tem uma variância maior ou igual ao seu sucessor, ou seja, ${\lambda _1} \ge {\lambda _2} \ge ... \ge {\lambda _N}$. Dessa forma, é possível selecionar os componentes de maior energia através dos autovalores ${\lambda _i}$.

\begin{figure}[h]
\subfigure[Exemplos de \textit{eigenfaces}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenfacesEx.png}
\label{fig:eigenfacesEx}
}
\subfigure[Exemplos de \textit{eigenpoints}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenPointsEx.png}
\label{fig:eigenpointsEx}}
\caption{Exemplos de \textit{eigenfaces}  à direita e a esquerda temos \textit{eigenpoints} extraídos a partir de blocos centrados a partir do olho direito das imagens das faces.}
\end{figure}

\subsection*{Formulação do IPD-B-PCA}

O método utilizado consiste em aumentar a robustez do classificador IPD através das componentes principais de maior energia, ou seja, cujos autovalores associados são maiores \cite{waldir}. Matematicamente podemos obter esses filtros da seguinte maneira: suponhamos uma  variável aleatória que pode ser classificadas nas classes \{${\mathcal{A}_1},\,{\mathcal{A}_2},\,...,\,{\mathcal{A}_N}$\} onde: 

\begin{itemize}
\item A classe ${\mathcal{A}_1}$ será composta pela componente que desejamos detectar (${\mathcal{A}_1=\{\phi _i\}}$);

\item A classe ${\mathcal{A}_2}$ refere-se a deslocamentos lineares de $\phi _i$. A expressão $\phi _i^{n,m}$ relaciona o deslocamento linear da componente \textit{i} em \textit{n} \textit{pixels} na direção horizontal e \textit{m} na vertical. Considerando-se blocos quadrados de tamanho \textit{L} em que o centro do bloco contém o ponto fiducial de interesse, podemos escrever ${\mathcal{A}_2} = \{ \phi _i^{n,m}$, onde: $- L \le \,m,n\, \le \,L$, com $n,m \ne \,0\}$ .

\item A classe $\mathcal{B}$ será composta pelas demais componentes $\phi _j$, ou seja, as classes \newline\{${\mathcal{A}_3},\,{\mathcal{A}_4},\,...,\,{\mathcal{A}_N}$\}.
\end{itemize}

Para projetarmos o nosso classificador utilizaremos a equação \eqref{vetorH_final}, levando-se em consideração as premissas citadas teremos:

\begin{equation}\label{eq_ipd_pca}
{h_{{\phi _1}}} = {\left\{ {{p_1}{R_{A1}} + {p_2}{R_{A2}} + {p_B}{R_B}} \right\}^{ - 1}}\,{p_1}{R_{A1}}
\end{equation}

A Figura \ref{fig_ipd_pca} ilustra o esquema proposto, onde temos \textit{S} \textit{eigenpoints} ($\Phi  = \,{\phi _1},\,{\phi _2},\,...,\,{\phi _S}$), para cada um será projetado um filtro, de forma que estejam dispostos do maior para o menor valor de variância (autovalores), desse modo, ${\lambda _1} \ge \,{\lambda _2} \ge ,...,\,{\lambda _S}$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.9]{figuras/esquema_Matriz.png}
\caption{\textit{S} classificadores para \textit{S} componentes principais.}
\label{fig_ipd_pca}
\end{figure}

Na Figura \ref{fig_schema}, podemos observar um \textit{eigenpoint} que refere-se ao olho direito considerando-se apenas a primeira componente principal. A componente $\phi_1$ foi calculada através de blocos de dimensão $2L+1$ (imagem à esquerda). Afim de obtermos a componente $\phi_2$ através de deslocamentos lineares de meio bloco calculamos $\mathop {{\phi _i}}\limits^ \sim$ para blocos de dimensão $4L+1$ (imagem à direita). Assim, reescrevendo a equação \eqref{eq_ipd_pca} em função dessas ponderações teremos:

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{figuras/Schema.png}
\caption{Desclocamento linear}
\label{fig_schema}
\end{figure}


\begin{equation} \label{eq_IPD-B-PCA_Final}
h{\phi _i} = {\left\{ \begin{array}{l}
{p_1}{\phi _i}{({\phi _i})^{T}} + {p_2}\frac{1}{{{{(2L + 1)}^2} - 1}}\,\sum\limits_{n =  - L\hfill\atop
n \ne 0\hfill}^L {\sum\limits_{m =  - L\hfill\atop
m \ne 0\hfill}^L {{\phi _i}^{(n,m)}({\phi _i}^{(n,m)})^{T} }} +\\
 + p(\mathcal{B})\,\frac{1}{{N - 1}}\,\sum\limits_{j = 1\hfill\atop
j \ne i\hfill}^N {{\phi _j}{{({\phi _j})}^{T}}} 
\end{array} \right\}^{ - 1}}\,{p_1}{\phi _i}
\end{equation}

Finalmente, a equação \eqref{eq_IPD-B-PCA_Final} expressa o detector por produto interno utilizando análise de componentes principais. Salienta-se que na construção dos padrões de interesse da classe $\mathcal{A}_1$ podemos utilizar componentes levemente rotacionados ou deslocados, dessa forma é possível construir detectores mais robustos. A classe ${\mathcal{A}_2}$ é de fundamental importância, uma vez que se considerássemos somente as classes dos padrões de interesse poderíamos discriminar um falso positivo, ou seja, a classe $\mathcal{A}_1$ deslocada. Esse fato, deve-se a linearidade dos filtros, portanto, pequenas alterações na entrada produzem apenas pequenas perturbações na saída. Assim, a adição da classe ${\mathcal{A}_2}$ permite ao classificador rejeitar pequenos deslocamentos lineares de forma mais precisa e com uma maior eficiência.
                      
\section{Computação Paralela}

A ideia central da computação paralela é de que um tarefa geralmente pode ser dividida em partes menores e executadas de forma concorrente. Atualmente existem duas abordagens no que tange a computação paralela, a primeira diz respeito aos \textit{multi-cores}, integra alguns núcleos (entre dois e dez) em um único microprocessador, como exemplo temos \textit{desktops}, \textit{smartphones}, etc. A segunda relaciona-se aos \textit{many-cores} e faz uso de um grande número de núcleos (geralmente várias centenas) e está especialmente orientada à execução de programas paralelos, o principal exemplo desse modelo são as GPUs (\textit{Graphics Processing Unit}) \cite{diaz2012survey}.

No que tange a arquitetura podemos destacar a quantidade de \textit{cores} já quanto a questão de \textit{software} para computação paralela podemos destacar algumas APIs\footnote{do inglês \textit{Application Programming Interface} é um conjunto de rotinas e padrões de programação para acesso a um aplicativo de software ou plataforma baseado na Web.}, as quais são baseadas no tipo de memória utilizada (compartilhada ou distribuída). Destaca-se \textit{POSIX Threads}, \textit{OpenMP} e MPI. A tabela \ref{tab_difPar} comenta as principais diferenças.

O OpenMP é uma API aplicado a arquiteturas de programação paralela de memória compartilhada, ela é orientada para a tarefa e trabalha em um nível de abstração mais elevado do que as \textit{threads}. Essa API é portável entre arquiteturas de memória compartilhada, e a transição dos trechos sequenciais-paralelos se dão através da estrutura \textit{fork/join}. O elevado nível de abstração e a facilidade da API tornam o OpenMP adequado para o desenvolvimento de aplicações de HPC em sistemas de memória compartilhada \cite{diaz2012survey}.

\textit{POSIX Threads} (do inglês, \textit{Portable Operating System Interface}), também conhecido como \textit{Pthreads} é um modelo baseado em memória compartilhada que padroniza o uso de \textit{threads} entre diferentes sistemas operativo/arquiteturas. \textit{Pthreads} são definidas como um conjunto de tipos de dados em C e um conjunto de rotinas.

O MPI (\textit{Message Passing}), é um modelo de programação paralela aplicado a memórias distribuídas que faz uso de troca de mensagens para a comunicação entre processos. No MPI diferentes processos podem executar diferentes programas, por isso também é relacionado como MPMD (\textit{multiple program multiple data}). A principal vantagem do MPI é portabilidade entre diferentes plataformas e a simplificação de tarefas através das rotinas implementadas.

\begin{center}
\begin{table}[tbh]
\centering
\newcolumntype{M}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand\arraystretch{1} \setlength\minrowclearance{2.4pt}
\caption{Modelos de Programação Paralela Puros}
\vspace{0.5cm}
\label{tab_difPar}
\begin{tabular}{|l|M{2cm}|M{3cm}|M{3cm}|}
\hline
\textbf{Implementação} & \textbf{Pthreads} & \textbf{OpenMP} & \textbf{MPI} \\ 
\hline
\textbf{Modelo de Programação} & Threads & Memória Compartilhada & Troca de mensagens \\ 
\hline
\textbf{Arquitetura do Sistema} & Memória Compartilhada & Memória Compartilhada & Memória Distribuída e Compartilhada \\ 
\hline
\textbf{Modelo de comunicação} & Endereçamento compartilhado & Endereçamento compartilhado & Troca de mensagens ou Endereçamento compartilhado \\ 
\hline
\textbf{Sincronização} & Explícita & Implícita & Implícita ou Explícita \\ 
\hline
\textbf{Implementação} & Biblioteca & Compilador & Biblioteca \\ 
\hline
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ 
\end{tabular}
\end{table}
\end{center}

Estudos a respeito do uso de núcleos de processamento (\textit{cores}) em telefones celulares com o Sistema Operacional Android demonstram que estes são usados de maneira ineficiente \cite{gao2015study} \cite{wu2013study}. Isso ocorre principalmente pela maneira de programação dos \textit{softwares}, a maioria não explora recusrsos de  paralelização de maneira eficiente, outro problema observado tange a paralelização implícita não ser tão eficaz devido a complexidade de um algoritmo transformar trechos de códigos sequenciais em paralelos. Inerentemente algumas partes do código são sequenciais (inicialização, instanciação de tarefas, por exemplo) e isso provoca um aumento no tempo de execução, porém os trechos que podem ser paralelizáveis anda são pouco explorados \cite{gao2015study}.

\section{Sistema IPD-PCA}

Para a implementação do sistema proposto levamos em consideração que o problema de aprendizagem é supervisionado, logo, temos que os padrões utilizados para o treinamento possuem uma classe pré-especificada. O sistema será dividido em duas etapas: treino e teste.

A etapa de treino será dividida da seguinte forma: Procedimentos iniciais de treinamento, projeto dos detectores $\rm h{\phi _i}$, em seguida é realizado o uma operação de produto interno e por último é realizado o treinamento do classificador. A etapa de teste é muito semelhante a de treino, ela será dividida da seguinte forma: procedimentos iniciais para treinamento, operação de produto interno com os detectores $\rm h{\phi _i}$ (obtidos no treinamento), classificação e pós-processamento. As fases de Procedimentos Iniciais para Treinamento são idênticas em ambas as fases, o esquema da Figura \ref{fig_System_Schema} ilustra o sistema, a seguir será detalhado cada uma das etapas.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{figuras/System_Diagram.png}
\caption{Diagrama em bloco so sistema.}
\label{fig_System_Schema}
\end{figure}

\subsection{Treino}\label{treino}

Nessa fase desenvolvemos o aprendizado de todos os modelos dos detectores IPD, para isso usaremos uma base de dados contendo faces humanas frontais em nível de cinza, as faces possuem pequenas variações de escala (perto e mais afastado da câmera), iluminação e pequenas rotações. A Figura \ref{fig_Train_Schema} ilustra o diagrama em blocos da etapa de treinamento, a seguir explanaremos sobre o sistema.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{figuras/Schema_Train.png}
\caption{Diagrama em bloco da etapa de treinamento.}
\label{fig_Train_Schema}
\end{figure}


\subsubsection{Procedimentos iniciais para o treinamento}\label{Pre-Proc}

A seguir detalharemos o primeiro bloco da Figura \ref{fig_Train_Schema}. Esse bloco é comum tanto a etapas de treinamento e teste, e conforme ilustrado na Figura \ref{fig_Pre-Proc}, é composto das seguintes passos: enquadramento do rosto através do método de \textit{Viola-Jones}, escalonamento da imagem, correção de iluminação, modelo gaussiano à priori (redução do espaço de busca) e definição das classes positivos e negativos. Sequencialmente cada uma das imagens passarão pelos seguintes procedimentos de modo a extrair as características desejadas para a construção de um modelo:

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.55]{figuras/PIT_Pre_Proc.png}
\caption{Diagrama em bloco da fase Procedimentos Iniciais de treinamento.}
\label{fig_Pre-Proc}
\end{figure}

\begin{itemize}
\item \textbf{Detector de faces VJ}: É realizado o enquadramento de face na imagem de entrada através do algoritmo de Viola-Jones \cite{viola2004robust};

\item \textbf{Escalonamento}: Em seguida a imagem é reescalada. Essa técnica é utilizada pois as diferentes imagens não tem a mesma distância da câmera;

\item \textbf{Correção de Iluminação}: A seguir é aplicada a uma correção de iluminação através da correção do fator gama, filtragem por diferença de gaussianas e equalização de contraste \cite{tan2010enhanced};

\item \textbf{Modelo Gaussiano à Priori}: Feita a correção de iluminação é definido uma redução no espaço de busca segundo um modelo probabilístico gaussiano;

\item \textbf{Definição das classes Positivos e Nagativos}: Finalmente serão definidas como positivos e negativos conforme explicado na seção \ref{cap-IPD-PCA}, onde as classes $\mathcal{A}_1$ será definida como positivo
por se tratar de blocos com centros em pontos fiduciais e as classes $\mathcal{A}_2$ e $\mathcal{B}$ serão definidas como negativos por tratarem de outros blocos.
\end{itemize}


\subsubsection{Procedimentos para Treinamento}

A seguir será explicado o funcionamento do restante dos blocos da Figura \ref{fig_Train_Schema}. Passado o bloco de procedimentos iniciais, serão projetados \textit{k} detectores $\rm h_{\phi _i}$ (\textit{i}={1,...,\textit{k}}), um para cada uma das componentes principais com maiores variâncias utilizando-se a Equação \ref{eq_IPD-B-PCA_Final}. Depois de encontrados os \textit{k} vetores é iniciado o treinamento do classificador AdaBoost\footnote{\textit{Adaptative Boosting}, é a técnica de aprendizado na qual podemos construir classificadores fortes a partir de uma combinação de classicadores fracos.}, que será responsável pela detecção das classes. Cada bloco pertencente as classes de positivos e negativos (saída do bloco de procedimentos iniciais para Treinamento) será subtraído de um bloco médio de todas as classes de positivos (denominado $\mu_{U}$), em seguida, serão processados utilizando-se o produto interno $\rm h_{\phi i}^{*T}$${B_Z}$. Desse modo, cada bloco ${B_Z}$ terá um vetor associado denominado $\rm d$$_{B_Z}$ com dimensões 1x\textit{k}. O conjunto de vetores $\rm d$$_{B_Z}$ serão utilizados no treinamento do classificador  de modo que possamos discriminar blocos positivos e negativos na outra etapa (teste).

\subsection{Teste}

A Figura \ref{fig_Schema_Test} ilustra o diagrama em blocos que representa a fase de teste para o sistema IPD-PCA, que será detalhado a seguir.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.78]{figuras/Schema_Test.png}
\caption{Treinamento dos classificadores $\rm 
h_{\phi _i}$.}
\label{fig_Schema_Test}
\end{figure}

Nessa etapa verifica-se o desempenho do sistema de detecção de pontos fiduciais, a descrição dessa etapa é feita a seguir. Inicialmente realizamos os Procedimentos Iniciais para Teste, utilizando o mesmo procedimento descrito na Seção \ref{Pre-Proc}, posteriormente utilizando uma janela deslizante selecionamos uma quantidade de blocos $B_{Z}$ aleatória em cada região de interesse por imagem (onde z é a coordenada correspondente ao centro do bloco). Em seguida processamos o produto interno dos detectores $\rm h_{\phi _i}$ (obtidos na etapa de treinamento) com $B_{Z}$ subtraído por $\mu_{U}$ (bloco médio de todas as classes de positivos, determinado na etapa de treino). Assim, cada bloco $B_{Z}$ terá um vetor $\rm d$$_{B_Z}$ associado, esse vetor  será usado para classificar $B_{Z}$. Em seguida é feito um pós-processamento conforme a estratégia escolhida.

A saída do classificador AdaBoost é uma nuvem de pontos, conforme pode ser visto da Figura \ref{Pre-Proc}. A partir dessa nuvem escolheremos apenas uma coordenada que será candidato eleito a ponto fiducial. Com base em \cite{waldir}, temos quatro estratégias discutidas a seguir:

\begin{itemize}
\item A estratégia de acrônimo \textit{NA} não realiza nenhuma modificação na saída do bloco AdaBoost. Assim, todos os elementos da nuvem são considerados saídas do método;

\item A segunda estratégia é denominada \textit{ML}, ela seleciona a coordenada mais provável, supondo o modelo gaussiano a priori como critério de decisão;

\item A estratégia \textit{GML} agrupa os elementos com distância menor que \textit{P} pixels e seleciona os resultados dos agrupamentos conforme \textit{ML};

\item A estratégia \textit{A} dá como saída do método a média dos elementos da nuvem;
\end{itemize}

\chapter{Fundamentos Específicos}\label{cap-especifico}

\section{Contextualização do Problema} \label{sec_Problem}

Diversos estudos acerca de desempenho computacional tem-se desenvolvido, uma dessas linhas de pesquisa diz respeito a utilização de \textit{cores} (ou núcleos de processamento). Trabalhos como de \cite{flautner2000thread}, \cite{gao2015study}, \cite{wu2013study} apontam que os \textit{cores}, de modo geral, são subutilizados e o incremento na quantidade de núcleos não traduz de forma proporcional em ganho de performance. As aplicações anteriores aos processadores \textit{multi-cores} utilizavam-se do paradigma sequencial de programação, e muitas aplicações atuais não exploram recursos de paralelização das tarefas \cite{flautner2000thread}.

Os sistemas embarcados por sua vez também acompanharam a evolução dos processadores e incorporaram a arquitetura \textit{multi-core}. A plataforma Android, amplamente utilizada em diversos sistemas embarcados, não explora de maneira eficiente todos os recursos do seu processador, fazendo assim com que alguns processos levem mais tempo para serem executados além de sobrecarregar outros \textit{cores} \cite{gao2015study}. Geralmente isso ocorre por causa do paralelismo implícito (no qual o próprio sistema operativo é encarregado de paralelizar trechos das aplicações), pois a paralelização de um programa é tarefa extremamente complexa e variável para um algoritmo fazer sempre de forma eficiente.

Embora o processamento dos sistemas embarcados estejam cada dia mais velozes, de um modo geral, o processamento digital de imagens ainda demanda por bastante poder de processamento. Se tomarmos como referência aplicações Android, perceberemos que a maioria não explora de maneira satisfatória todos os recursos do seus núcleos, nem mesmo as aplicações nativas (desenvolvidos pelo próprio Google), conforme afirma \cite{gao2015study}. Esse fato aliado a pouca quantidade de memória RAM (do inglês, \textit{Random Access Memory}) que possui a maioria desses dispositivos nos permite inferir que há uma sobrecarga de processamento em aplicações mais pesadas como, por exemplo, as que detectam pontos fiduciais e/ou reconhecimento facial.

A ideia central desse trabalho é de identificar as estruturas paralelizáveis no modelo IPD-PCA proposto por \cite{waldir} em um \textit{smartphone} de plataforma Android e processá-las usando técnicas de computação paralela. Nas seções posteriores serão detalhados as abordagens e modificações no modelo.


\section{Modelo Proposto} \label{sec_ModeloProposto}

As Figuras \ref{fig:ParalelismoTreino} e \ref{fig:ParalelismoTeste} ilustram as modificações na etapa de treino e de teste, respectivamente, propostas nesse trabalho. Conforme podemos perceber as fases de Procedimentos Iniciais para Treinamento é a mesma para ambas as etapas (Treino e Teste), ela constitui-se dos procedimentos mostrados na seção \ref{Pre-Proc} de forma sequencial, sendo que cada bloco depende do resultado do bloco anterior, desse modo não há meios de paralelizá-lo.

O bloco Projeto de $\rm h_{\phi _i}$, por sua vez é uma das estruturas as quais se propõe paralelização. Será construído um classificador para cada uma das componentes principais, o número de componentes principais na maioria dos pontos fiduciais definidos na marcação de \cite{waldir2010facial} foi de \textit{N}=63, com \textit{N}=53 nos demais pontos.

\begin{figure}[h]
\subfigure[Esquema da paralelização proposta para o bloco de Treino]{
\includegraphics[scale=0.64]{figuras/Parallelism_Block_Train.png}
\label{fig:ParalelismoTreino}
}
\subfigure[Esquema da paralelização proposta para o bloco de Teste]{
\includegraphics[scale=0.72]{figuras/Parallelism_Block_Test.png}
\label{fig:ParalelismoTeste}}
\caption{Esquema de modificações proposto para as etapas de Treino e de Teste}
\end{figure}

\section{Avaliações de Desempenho} \label{sec_Desempenho}

Aqui eu tô pensando em usar TLP (\textit{Thread Level Parallelism}) pra comparar o método:  sem usar paralelismo x com paralelismo. Acho que medir tempo também seria uma boa.


