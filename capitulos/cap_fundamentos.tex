\chapter{Fundamentos Teóricos}\label{cap-fundamentos}

\section{Detectores por Produto Interno com Minimização do Erro Quadrático Médio (IPD)} \label{sec_IPD}
               
O projeto do classificador utilizando detector por produto interno pode ser feito da seguinte forma: suponhamos uma variável aleatória, ou seja, uma variável cujo os seus resultados sejam imprevisíveis, $X_{dx1}$. Suas realizações podem ser classificadas nas classes {${A_1},\,{A_2},\,...,\,{A_n}$} ou na classe B, sendo que a classe ${A_i}$ representa os padrões que desejamos encontrar e B representa os demais padrões que não seja de interesse. As probabilidades de encontrarmos essas classes são dadas por:
                 
\begin{equation}\label{eq_prob}
\begin{array}{l}
p({A_i}) = p(\mathcal{X} \in {A_i}) = {p_i}\\
p(B) = p(\mathcal{X} \in B)
\end{array}
\end{equation}

O classificador \textbf{h} é projetado de tal forma que o produto interno dele com uma entrada $\mathcal{X}$ seja dado por:
                
\begin{equation}\label{eq_probabilidades}
<\mathcal{X}, h_{A_{i}}>=h^{t}_{A_{i}}\mathcal{X}=C 
\end{equation}
                
Onde $\mathcal{C}$=1 caso $\mathcal{X} \in A_i$  e $\mathcal{C}=0$ para $\mathcal{X} \in A_{i}$.
                
Se tomarmos o problema dos mínimos quadrados que tenta minimizar a soma dos quadrados das diferenças entre o valor estimado e os dados observados, a equação \eqref{eq_probabilidades} procura encontrar o melhor classificador possível. Sendo assim, podemos definir o erro como:

\begin{equation}
\varepsilon=h^{t}_{Ai}\mathcal{X}-\mathcal{C}
\end{equation}
        
Assim, considerando-se definição de erro quadrático temos:
                
\begin{equation}
||\varepsilon||^{2}=(h^{t}_{Ai}\mathcal{X}-\mathcal{C})(h^{t}_{Ai} \mathcal{X}-\mathcal{C})^{t}\label{eq_erroQuad}
\end{equation}
                
Assumindo que \textit{$h_{A_{i}}$}, $\mathcal{X}$ e $\mathcal{C}$ como valores reais e desenvolvendo  a equação \eqref{eq_erroQuad} chegaremos ao valor esperado do erro quadrático como sendo:
                

\begin{equation} \label{eq_erroQuadratico}
E[||\varepsilon|{|^2}] = h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]
\end{equation}
                
Afim de minimizarmos o erro quadrático da equação \eqref{eq_erroQuadratico} igualamos a zero (0) o seu gradiente em relação a \textit{$h_{A_{i}}$}. Dessa forma, obtemos a seguinte expressão:
                
\[
\begin{array}{l}
\frac{\partial E[||\varepsilon |{|^2}]}{{\partial {h_{Ai}}}} = \frac{\partial }{{\partial {h_{Ai}}}}\{ h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]\} \\
0 = \{ E[\mathcal{X}{\mathcal{X}^t}] + E{[\mathcal{X}{\mathcal{X}^t}]^t}\} {h_{Ai}} - 2E[\mathcal{XC}]\\
2E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2E[\mathcal{XC}] = 0
\end{array}
\]
                
Logo,

\begin{equation}\label{vetorH}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} = E[\mathcal{XC}]\\
{h_{Ai}} = {\{ E[\mathcal{X}{\mathcal{X}^t}]\} ^{ - 1}}E[\mathcal{XC}]
\end{array}
\end{equation}
                
Os termos $E[\mathcal{X}{\mathcal{X}^t}]$ e $E[\mathcal{XC}]$ até então são desconhecidos. Podemos desenvolver o termo $E[\mathcal{X}{\mathcal{X}^t}]$ da seguinte forma:

\begin{equation}\label{eqSeiSete}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})
\end{equation}

Lembrando o iníco da seção onde definimos as classes ${A_1},\,{A_2},\,...,\,{A_n}$ e a classe B onde podemos encontrar ${B_1},\,{B_2},\,...,\,{B_n}$, podemos encontrar as médias da equação \eqref{eqSeiSete} da seguinte forma:

\begin{equation}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})\\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {p({A_j})\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} \\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} 
\end{array}
\end{equation}

Conforme verificamos na equação \eqref{eq_probabilidades}, obteremos a saída igual a 1 para o caso da variável aleatória X for classificada na classe $A_i$ e 0 caso contrário. Seguindo o raciocínio desenvolvido para a classe A temos as classes representadas através do complemento de $A_i$, dado por $A_i^C$:

\begin{equation}
\begin{array}{l}
E[\mathcal{XC}] = E[\mathcal{XC}|{A_i}]p({A_i}) + E[\mathcal{XC}|A_i^C](1 - p({A_i}))\\
 = E[\mathcal{X}|{A_i}]p({A_i}) + 0(1 - p({A_i}))\\
 = {p_i}\frac{1}{{{L_1}}}\sum\limits_{k = 1}^{{L_1}} {{A_{ik}}} 
\end{array}
\end{equation} 

Assim, a equação \eqref{vetorH}, pode ser reescrita da forma:

\begin{equation}
{h_{Ai}} = {\{ p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t}  + \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}\sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} } \} ^{ - 1}}\{ {p_i}\frac{1}{{{L_i}}}\sum\limits_{k = 1}^{{L_i}} {{A_{ik}}} \}
\end{equation}

Considerando-se a matriz de autocorrelação de $B$ igual a $R_B$ e a matriz $A_i$ igual a $R_{Ai}$, e a média $A_i$ igual a $\mu_{Ai}$, podemos expressar os resultados em função das variáveis aleatórias, assim, temos:

\begin{equation}\label{vetorH_final}
{h_{Ai}} = {\{ p(B){R_B} + \sum\limits_{j = 1}^n {{p_j}{R_{Aj}}} \} ^{ - 1}}\{\sum\limits_{i = 1}^m {{p_i}{\mu _{Ai}}}\}
\end{equation}

A condição de existência do classificador projetado é que o primeiro termo da equação \eqref{vetorH_final} seja uma matriz inversível e a dimensão dos vetores de entrada deve ser menor que a soma dos números de elementos de todas as classes $A_j$ e B, ou seja $d \le \,r\, + \,\sum\limits_{j = 1}^n {{L_j}}$.
              
\section{Detector por Produto Interno utilizando Análise de Componentes Principais}\label{cap-IPD-PCA}

Com o intuito de aumentar a robustez do detector por produto interno (IPD) a pequenas rotações, utilizou-se a técnica de análise de componentes principais (do inglês, \textit{principal component analysis} - PCA) \cite{waldir}. Essa técnica baseia-se na extração de um número desejado de componentes principais de um vetor multidimensional, de modo que ao final da operação tenhamos um vetor cuja dimensão é inferior à original e sua variância seja maximizada \cite{waldir}. A seguir comentaremos sobre o método denominado \textit{eigenpoint}, que é utilizado para encontrar pontos fiduciais\footnote{do inglês, \textit{fiducial point}, refere-se a características salientes comum as faces, como canto da boca, olho, nariz, etc.}

\subsection*{\textit{Eigenpoints}}

O termo \textit{eigenpoints} refere-se a um método que faz uso de PCA para a detecção de pontos fiduciais em imagens. A análise de componentes principais, é associada à ideia de redução de dimensionalidade,  com  menor  perda  possível  da  informação. Tendo em vista a similaridade dos pontos fiduciais humanos bem como a localização destes, utiliza-se essa técnica para representá-los em um subespaço de dimensão inferior ou igual ao original, assim, podemos utilizar \textit{eigenpoints} para encontrarmos os pontos fiduciais de um conjunto de faces, por exemplo.

 \textit{Eigenpoints} constitui-se um caso específico de \textit{eigenfaces} (Figura \ref{fig:eigenfacesEx}), porém, ao invés de utilizarmos toda a face tomamos um ponto fiducial de interesse e recortamos o bloco em que ele está contido, conforme pode ser visto na Figura \ref{fig:eigenpointsEx} (na Figura \ref{fig:eigenfacesEx} temos o \textit{eigenface} e na Figura \ref{fig:eigenpointsEx} recortamos o bloco no qual está contido o olho direito - \textit{eigenpoint}).

Matematicamente, podemos obter os \textit{eigenpoints} como sendo $M$ realizações de uma variável aleatória $\mathcal{X}_{N \times1}$ iguais a x$_{\textit{i}}$. Cada coluna x$_{\textit{i}}$ contém os \textit{pixels} de blocos centrados em pontos fiduciais ${\gamma_\textit{i}}$, onde \textit{i} = \{1,..., M\}. Dessa forma, \textbf{X} é uma matriz formada por \textit{M} vetores (sendo \textit{M} > \textit{N}) da seguinte forma:

\begin{equation}
\textbf{X} = [{\rm{x}_1} - {\mu _\mathcal{X}},\,...,\,{\rm{x}_M} - {\mu _\mathcal{X}}]
\end{equation}

onde $\mu _\mathcal{X}$ é:

\begin{equation}
{\mu _\mathcal{X}} = \frac{1}{M}\sum\limits_{i = 1}^M {{\rm{x}_\textit{i}}}
\end{equation}

Seja \textbf{$\mathbf{\Sigma}_\mathcal{X}$} a matriz de covariância de $\mathcal{X}$, dada por:

\begin{equation}
\mathbf{\Sigma}_\mathcal{X}  =  \frac{1}{{M - 1}}\textbf{X}{\textbf{X}^{*T}}
\end{equation}

Dessa forma, a base que contém as \textit{N} componentes principais descorrelacionadas (\textit{eingenpoints}) é encontrada através da diagonalização da matriz $\mathbf{\Sigma}_\mathcal{X}$, assim:

\begin{equation}
\mathbf{\Lambda} = \mathbf{\Phi}^{*t} \mathbf{\Sigma}_{\mathcal{X}} \mathbf{\Phi}.
\end{equation}

sendo $\Phi  = [{\phi _1},\,{\phi _2},\,...,\,{\phi _N}]$ é a matriz de autovetores de $\mathbf{\Sigma}_\mathcal{X}$, o sobrescrito \textit{T} indica o transposto da matriz de covariância e $\Lambda$ é a matriz diagonal contendo os valores de $\sum\nolimits_\mathcal{X}$, onde cada autovalor ${\lambda _\textit{i}}$ tem uma variância maior ou igual ao seu sucessor, ou seja, ${\lambda _1} \ge {\lambda _2} \ge ... \ge {\lambda _N}$. Dessa forma, é possível selecionar os componentes de maior energia através dos autovalores ${\lambda _i}$.

\begin{figure}[h]
\subfigure[Exemplos de \textit{eigenfaces}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenfacesEx.png}
\label{fig:eigenfacesEx}
}
\subfigure[Exemplos de \textit{eigenpoints}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenPointsEx.png}
\label{fig:eigenpointsEx}}
\caption{Exemplos de \textit{eigenfaces}  à direita e a esquerda temos \textit{eigenpoints} extraídos a partir de blocos centrados a partir do olho direito das imagens das faces.}
\end{figure}

\subsection*{Formulação do IPD-B-PCA}

O método utilizado consiste em aumentar a robustez do classificador IPD através das componentes principais de maior energia, ou seja, cujos autovalores associados são maiores \cite{waldir}. Matematicamente podemos obter esses filtros da seguinte maneira: suponhamos uma  variável aleatória que pode ser classificadas nas classes \{${\mathcal{A}_1},\,{\mathcal{A}_2},\,...,\,{\mathcal{A}_N}$\} onde: 

\begin{itemize}
\item A classe ${\mathcal{A}_1}$ será composta pela componente que desejamos detectar (${\mathcal{A}_1=\{\phi _i\}}$);

\item A classe ${\mathcal{A}_2}$ refere-se a deslocamentos lineares de $\phi _i$. A expressão $\phi _i^{n,m}$ relaciona o deslocamento linear da componente \textit{i} em \textit{n} \textit{pixels} na direção horizontal e \textit{m} na vertical. Considerando-se blocos quadrados de tamanho \textit{L} em que o centro do bloco contém o ponto fiducial de interesse, podemos escrever ${\mathcal{A}_2} = \{ \phi _i^{n,m}$, onde: $- L \le \,m,n\, \le \,L$, com $n,m \ne \,0\}$ .

\item A classe $\mathcal{B}$ será composta pelas demais componentes $\phi _j$, ou seja, as classes \newline\{${\mathcal{A}_3},\,{\mathcal{A}_4},\,...,\,{\mathcal{A}_N}$\}.
\end{itemize}

Para projetarmos o nosso classificador utilizaremos a equação \eqref{vetorH_final}, levando-se em consideração as premissas citadas teremos:

\begin{equation}\label{eq_ipd_pca}
{h_{{\phi _1}}} = {\left\{ {{p_1}{R_{A1}} + {p_2}{R_{A2}} + {p_B}{R_B}} \right\}^{ - 1}}\,{p_1}{R_{A1}}
\end{equation}

A Figura \ref{fig_ipd_pca} ilustra o esquema proposto, onde temos \textit{S} \textit{eigenpoints} ($\Phi  = \,{\phi _1},\,{\phi _2},\,...,\,{\phi _S}$), para cada um será projetado um filtro, de forma que estejam dispostos do maior para o menor valor de variância (autovalores), desse modo, ${\lambda _1} \ge \,{\lambda _2} \ge ,...,\,{\lambda _S}$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.9]{figuras/esquema_Matriz.png}
\caption{\textit{S} classificadores para \textit{S} componentes principais.}
\label{fig_ipd_pca}
\end{figure}

Na Figura \ref{fig_schema}, podemos observar um \textit{eigenpoint} que refere-se ao olho direito considerando-se apenas a primeira componente principal. A componente $\phi_1$ foi calculada através de blocos de dimensão $2L+1$ (imagem à esquerda). Afim de obtermos a componente $\phi_2$ através de deslocamentos lineares de meio bloco calculamos $\mathop {{\phi _i}}\limits^ \sim$ para blocos de dimensão $4L+1$ (imagem à direita). Assim, reescrevendo a equação \eqref{eq_ipd_pca} em função dessas ponderações teremos:

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{figuras/Schema.png}
\caption{Desclocamento linear}
\label{fig_schema}
\end{figure}


\begin{equation} \label{eq_IPD-B-PCA_Final}
h{\phi _i} = {\left\{ \begin{array}{l}
{p_1}{\phi _i}{({\phi _i})^{T}} + {p_2}\frac{1}{{{{(2L + 1)}^2} - 1}}\,\sum\limits_{n =  - L\hfill\atop
n \ne 0\hfill}^L {\sum\limits_{m =  - L\hfill\atop
m \ne 0\hfill}^L {{\phi _i}^{(n,m)}({\phi _i}^{(n,m)})^{T} }} +\\
 + p(\mathcal{B})\,\frac{1}{{N - 1}}\,\sum\limits_{j = 1\hfill\atop
j \ne i\hfill}^N {{\phi _j}{{({\phi _j})}^{T}}} 
\end{array} \right\}^{ - 1}}\,{p_1}{\phi _i}
\end{equation}

Finalmente, a equação \eqref{eq_IPD-B-PCA_Final} expressa o detector por produto interno utilizando análise de componentes principais. Salienta-se que na construção dos padrões de interesse da classe $\mathcal{A}_1$ podemos utilizar componentes levemente rotacionados ou deslocados, dessa forma é possível construir detectores mais robustos. A classe ${\mathcal{A}_2}$ é de fundamental importância, uma vez que se considerássemos somente as classes dos padrões de interesse poderíamos discriminar um falso positivo, ou seja, a classe $\mathcal{A}_1$ deslocada. Esse fato, deve-se a linearidade dos filtros, portanto, pequenas alterações na entrada produzem apenas pequenas perturbações na saída. Assim, a adição da classe ${\mathcal{A}_2}$ permite ao classificador rejeitar pequenos deslocamentos lineares de forma mais precisa e com uma maior eficiência.
                      
\section{Computação Paralela}

A ideia central da computação paralela é de que um tarefa geralmente pode ser dividida em partes menores e executadas de forma concorrente. Atualmente existem duas abordagens no que tange a computação paralela, a primeira diz respeito aos \textit{multi-cores}, integra alguns núcleos (entre dois e dez) em um único microprocessador, como exemplo temos \textit{desktops}, \textit{smartphones}, etc. A segunda relaciona-se aos \textit{many-cores} e faz uso de um grande número de núcleos (geralmente várias centenas) e está especialmente orientada à execução de programas paralelos, o principal exemplo desse modelo são as GPUs (\textit{Graphics Processing Unit}) \cite{diaz2012survey}.

No que tange a arquitetura podemos destacar a quantidade de \textit{cores} já quanto a questão de \textit{software} para computação paralela podemos destacar algumas APIs\footnote{do inglês \textit{Application Programming Interface} é um conjunto de rotinas e padrões de programação para acesso a um aplicativo de software ou plataforma baseado na Web.}, as quais são baseadas no tipo de memória utilizada (compartilhada ou distribuída). Destaca-se \textit{POSIX Threads}, \textit{OpenMP} e MPI. A tabela \ref{tab_difPar} comenta as principais diferenças.

O OpenMP é uma API aplicado a arquiteturas de programação paralela de memória compartilhada, ela é orientada para a tarefa e trabalha em um nível de abstração mais elevado do que as \textit{threads}. Essa API é portável entre arquiteturas de memória compartilhada, e a transição dos trechos sequenciais-paralelos se dão através da estrutura \textit{fork/join}. O elevado nível de abstração e a facilidade da API tornam o OpenMP adequado para o desenvolvimento de aplicações de HPC em sistemas de memória compartilhada \cite{diaz2012survey}.

\textit{POSIX Threads} (do inglês, \textit{Portable Operating System Interface}), também conhecido como \textit{Pthreads} é um modelo baseado em memória compartilhada que padroniza o uso de \textit{threads} entre diferentes sistemas operativo/arquiteturas. \textit{Pthreads} são definidas como um conjunto de tipos de dados em C e um conjunto de rotinas.

O MPI (\textit{Message Passing}), é um modelo de programação paralela aplicado a memórias distribuídas que faz uso de troca de mensagens para a comunicação entre processos. No MPI diferentes processos podem executar diferentes programas, por isso também é relacionado como MPMD (\textit{multiple program multiple data}). A principal vantagem do MPI é portabilidade entre diferentes plataformas e a simplificação de tarefas através das rotinas implementadas.

\begin{center}
\begin{table}[tbh]
\centering
\newcolumntype{M}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand\arraystretch{1} \setlength\minrowclearance{2.4pt}
\caption{Modelos de Programação Paralela Puros}
\vspace{0.5cm}
\label{tab_difPar}
\begin{tabular}{|l|M{2cm}|M{3cm}|M{3cm}|}
\hline
\textbf{Implementação} & \textbf{Pthreads} & \textbf{OpenMP} & \textbf{MPI} \\ 
\hline
\textbf{Modelo de Programação} & Threads & Memória Compartilhada & Troca de mensagens \\ 
\hline
\textbf{Arquitetura do Sistema} & Memória Compartilhada & Memória Compartilhada & Memória Distribuída e Compartilhada \\ 
\hline
\textbf{Modelo de comunicação} & Endereçamento compartilhado & Endereçamento compartilhado & Troca de mensagens ou Endereçamento compartilhado \\ 
\hline
\textbf{Sincronização} & Explícita & Implícita & Implícita ou Explícita \\ 
\hline
\textbf{Implementação} & Biblioteca & Compilador & Biblioteca \\ 
\hline
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ 
\end{tabular}
\end{table}
\end{center}

Estudos a respeito do uso de núcleos de processamento (\textit{cores}) em telefones celulares com o Sistema Operacional Android demonstram que estes são usados de maneira ineficiente \cite{gao2015study} \cite{wu2013study}. Isso ocorre principalmente pela maneira de programação dos \textit{softwares}, a maioria não explora recusrsos de  paralelização de maneira eficiente, outro problema observado tange a paralelização implícita não ser tão eficaz devido a complexidade de um algoritmo transformar trechos de códigos sequenciais em paralelos. Inerentemente algumas partes do código são sequenciais (inicialização, instanciação de tarefas, por exemplo) e isso provoca um aumento no tempo de execução, porém os trechos que podem ser paralelizáveis anda são pouco explorados \cite{gao2015study}.

\section{Sistema IPD-PCA}

Para a implementação do sistema proposto levamos em consideração que o problema de aprendizagem é supervisionado, logo, temos que os padrões utilizados para o treinamento possuem uma classe pré-especificada. O sistema será dividido em duas etapas: aprendizado (treino) e operação (teste).

Na fase de aprendizado será dividido em quatro etapas: conjunto de treino, pré-processamento, extração de características e modelo. A fase de teste, por sua vez, será dividida em seis etapas: Meio físico, sensor, pré-processamento, extração de características, classificador e modelo. As etapas de pré-processamento e extração de características são idênticas em ambas as fases.

\subsection{Pré-Processamento}\label{Pre-Proc}

Nessa fase temos um conjunto de imagem de onde serão extraídas as características desejadas para a construção de um modelo. Sequencialmente cada uma das imagens passarão pelos seguintes procedimentos:

\begin{itemize}
\item É realizado o enquadramento de face na imagem através do algoritmo de Viola-Jones \cite{viola2004robust};

\item Em seguida a imagem é reescalada para uma dimensão pré-definida 220x220;

\item A seguir é aplicada a uma correção de iluminação através da correção do fator gama, filtragem por diferença de gaussianas e equalização de contraste \cite{tan2010enhanced};

\end{itemize}

Na Figura \ref{fig_PreProc_schema} temos a ilustração do esquema do bloco de pré-processamento e todas as sua fases. 

\begin{figure}[!htb]
\centering
\includegraphics[scale=1.0]{figuras/preProc_schema.png}
\caption{Bloco de Pré-Processamento}
\label{fig_PreProc_schema}
\end{figure}


\subsection{Treinamento}

Na fase de treinamento utiliza-se dois procedimentos iniciais, são eles:  região de interesse e definição das classes:

\subsubsection*{Região de Interesse}

A saída desse bloco de Pré Processamento será uma imagem de dimensão 220x220 (conforme mostrado na Figura \ref{fig_PreProc_schema}) e será a entrada no bloco de treino. Primeiramente iremos reduzir o espaço de busca de modo a se obter a região de interesse - \textit{Region Of Interest} (ROI) - através de um modelo probabilístico gaussiano denominado \textit{Modelo Gaussiano à Priori}. Essa zona se refere a uma região elíptica que é obtida da seguinte forma: seja $\mathcal{X}$ uma variável aleatória  no \textit{N} realizações iguais à coordenadas $\rm x_{i}$ do conjunto de imagens de treinamento (onde \textit{i}={1, 2,..., N}), a região de interesse será obtido através da função de densidade de probabilidade gaussiana com média $\mu_\mathcal{X}$ e matriz de covariância \textbf{$\mathbf{\Sigma}_\mathcal{X}$}. Calcula-se para todas as realizações de $\rm x_{i}$ o valor máximo da distância euclidiana entre $\mu_\mathcal{X}$ e $\rm x_{i}$. A esse valor será acrescentado 5\% como uma margem de segurança, e assim obteremos a região de interesse que pode ser observada na Figura \ref{fig_ROI}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{figuras/ROI.png}
\caption{Definição da região de interesse a partir do bloco de pré-processamento.}
\label{fig_ROI}
\end{figure}

\subsubsection*{Definição das Classes}

As classes serão classificada entre positivos quando se tratar de blocos com centros em pontos fiduciais e negativos quando se referir a outros blocos. De acordo com o que foi definido na Seção \ref{cap-IPD-PCA} são consideradas três classes por IPD, a classificação em positivos e negativos dar-se-á da seguinte forma:

\begin{itemize}
\item A classe $\mathcal{A}_{1}$ será definida como positivo;

\item A classe $\mathcal{A}_{2}$ e $\mathcal{B}$ será definida como negativo;
\end{itemize}


\begin{figure}[!htb]
\centering
\includegraphics[scale=1.5]{figuras/Classes.png}
\caption{O bloco centrado sobre o canto direito do olho esquerdo marcado como positivo terá suas componentes principais extraídas para treinamento de um IPD $h_{\Phi}$, as demais são classificadas como negativas}
\label{fig_Classes}
\end{figure}

\subsubsection*{Procedimentos para Treinamento}

A etapa de treinamento terá como saída terá como saída os vetores $h_{\phi _i}$ e os classificadores AdaBoost\footnote{\textit{Adaptative Boosting}, é a técnica de aprendizado na qual podemos construir classificadores fortes a partir de uma combinação de classicadores fracos.}\cite{cornelius2007efficient}. Os vetores $h_{\phi _1}$ serão criados conforme mostrado na Seção \ref{cap-IPD-PCA}, e os classificadores serão responsáveis por receber as imagens do conjunto de validação e retornar os candidatos a pontos fiduciais.

Para obtermos os classificadores executaremos todos os passos descritos anteriormente, são eles:

\begin{itemize}
\item Pré-Processamento: onde é detectada a face, reescalada, corrigida a iluminação;

\item Região de Interesse: em seguida a imagem é segmentada, afim de se obter a região de interesse;

\item Finalmente são definidos as classes como \textit{Positivo} e \textit{Negativo};
\end{itemize}

Posteriormente é construído o classificador local $h_{\phi _i}$ para a \textit{i}-ésima componente principal (\textit{i}=1, 2,...N) do bloco que tem como centro o ponto fiducial desejado utilizando-se a Equação \ref{eq_IPD-B-PCA_Final}. Depois de encontrados os \textit{N} vetores $h_{\phi _N}$ é iniciado o treinamento do classificador AdaBoost, que será responsável pela detecção das classes em positivos e negativos. Essa separação se dá através do produto interno entre os IPDs ($h_{\phi _i}$) e os blocos $B_{Z}$ de positivos e negativos (onde z refere-se a coordenada referente ao centro do bloco.)

O diagrama final do procedimento de treinamento é dado na Figura \ref{fig_Train_Schema}. Para o treinamento do sistema desenvolvido em \cite{waldir}, o número de componentes principais na maioria dos pontos fiduciais definidos na marcação foi de N = 63, com N = 53 nos demais pontos.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figuras/Train_Schema.png}
\caption{Treinamento dos classificadores $h_{\phi _i}$.}
\label{fig_Train_Schema}
\end{figure}

\subsection{Teste}

Nessa fase  verifica-se o desempenho so sistema de detecção de pontos fiduciais, o diagrama em blocos dessa etapa pode ser observado na Figura \ref{fig_Test}. Podemos descrever essa etapa da seguinte forma: \textit{i}) realizamos o pré-processamento, utilizando o mesmo procedimento descrito na Seção \ref{Pre-Proc}; \textit{ii}) as imagens serão processadas por uma janela deslizante $B_{Z}$ (onde z é a coordenada correspondente ao centro do bloco); \textit{iii}) acrescentamos uma dimensão extra em $B_{Z}$ utilizando $E_{m}$ (máxima norma) e subtraindo por $\mu$ (média global), o vetor resultante será denominado $b_{Z}$. Dessa forma, cada bloco $B_{Z}$ terá um correspondente $b_{Z}$; \textit{iv}) A seguir, cada $b_{Z}$ será classificado pela cascata de
classificadores IPD determinados no treinamento; \textit{v}) Realiza-se a etapa de pós-processamento;

A saída anterior so pós-processamento é uma nuvem de pontos, coordenadas no plano da imagem de prova reescalonada. A partir dessa nuvem escolheremos apenas uma coordenada que será candidato eleito a ponto fiducial. Com base em \cite{waldir}, temos quatro estratégias discutidas a seguir:

\begin{itemize}
\item A estratégia de acrônimo \textit{NA} não realiza nenhuma modificação na saída do bloco AdaBoost. Assim, todos os elementos da nuvem são considerados saídas do método;

\item A segunda estratégia é denominada \textit{ML}, ela seleciona a coordenada mais provável, supondo o modelo gaussiano a priori como critério de decisão;

\item A estratégia \textit{GML} agrupa os elementos com distância menor que \textit{P} pixels e seleciona os resultados dos agrupamentos conforme \textit{ML};

\item A estratégia \textit{A} dá como saída do método a média dos elementos da nuvem;
\end{itemize}

A proposta apresentada propõe como estratégia de pós-processamento a \textit{NA}, ou seja, todos os rótulos automáticos serão considerados como saída do sistema de
detecção de pontos fiduciais por IPD-PCA.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.48]{figuras/Test.png}
\caption{Treinamento dos classificadores $h_{\phi _i}$.}
\label{fig_Test}
\end{figure}


\chapter{Fundamentos Específicos}\label{cap-especifico}

\section{Contextualização do Problema} \label{sec_IPD}

