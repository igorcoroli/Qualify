\chapter{Fundamentos Teóricos}\label{cap-fundamentos}

\section{Detectores por Produto Interno com Minimização do Erro Quadrático Médio (IPD)} \label{sec_IPD}
               
O projeto do classificador utilizando detector por produto interno pode ser feito da seguinte forma: suponhamos uma variável aleatória, ou seja, uma variável cujo os seus resultados sejam imprevisíveis, $X_{dx1}$. Suas realizações podem ser classificadas nas classes {${A_1},\,{A_2},\,...,\,{A_n}$} ou na classe B, sendo que a classe ${A_i}$ representa os padrões que desejamos encontrar e B representa os demais padrões que não seja de interesse. As probabilidades de encontrarmos essas classes são dadas por:
                 
\begin{equation}\label{eq_prob}
\begin{array}{l}
p({A_i}) = p(\mathcal{X} \in {A_i}) = {p_i}\\
p(B) = p(\mathcal{X} \in B)
\end{array}
\end{equation}

O classificador \textbf{h} é projetado de tal forma que o produto interno dele com uma entrada $\mathcal{X}$ seja dado por:
                
\begin{equation}\label{eq_probabilidades}
<\mathcal{X}, h_{A_{i}}>=h^{t}_{A_{i}}\mathcal{X}=C 
\end{equation}
                
Onde $\mathcal{C}$=1 caso $\mathcal{X} \in A_i$  e $\mathcal{C}=0$ para $\mathcal{X} \in A_{i}$.
                
Se tomarmos o problema dos mínimos quadrados que tenta minimizar a soma dos quadrados das diferenças entre o valor estimado e os dados observados, a equação \eqref{eq_probabilidades} procura encontrar o melhor classificador possível. Sendo assim, podemos definir o erro como:

\begin{equation}
\varepsilon=h^{t}_{Ai}\mathcal{X}-\mathcal{C}
\end{equation}
        
Assim, considerando-se definição de erro quadrático temos:
                
\begin{equation}
||\varepsilon||^{2}=(h^{t}_{Ai}\mathcal{X}-\mathcal{C})(h^{t}_{Ai} \mathcal{X}-\mathcal{C})^{t}\label{eq_erroQuad}
\end{equation}
                
Assumindo que \textit{$h_{A_{i}}$}, $\mathcal{X}$ e $\mathcal{C}$ como valores reais e desenvolvendo  a equação \eqref{eq_erroQuad} chegaremos ao valor esperado do erro quadrático como sendo:
                

\begin{equation} \label{eq_erroQuadratico}
E[||\varepsilon|{|^2}] = h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]
\end{equation}
                
Afim de minimizarmos o erro quadrático da equação \eqref{eq_erroQuadratico} igualamos a zero (0) o seu gradiente em relação a \textit{$h_{A_{i}}$}. Dessa forma, obtemos a seguinte expressão:
                
\[
\begin{array}{l}
\frac{\partial E[||\varepsilon |{|^2}]}{{\partial {h_{Ai}}}} = \frac{\partial }{{\partial {h_{Ai}}}}\{ h_{Ai}^tE[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2h_{Ai}^tE[\mathcal{XC}] + E[{\mathcal{C}^2}]\} \\
0 = \{ E[\mathcal{X}{\mathcal{X}^t}] + E{[\mathcal{X}{\mathcal{X}^t}]^t}\} {h_{Ai}} - 2E[\mathcal{XC}]\\
2E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} - 2E[\mathcal{XC}] = 0
\end{array}
\]
                
Logo,

\begin{equation}\label{vetorH}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}]{h_{Ai}} = E[\mathcal{XC}]\\
{h_{Ai}} = {\{ E[\mathcal{X}{\mathcal{X}^t}]\} ^{ - 1}}E[\mathcal{XC}]
\end{array}
\end{equation}
                
Os termos $E[\mathcal{X}{\mathcal{X}^t}]$ e $E[\mathcal{XC}]$ até então são desconhecidos. Podemos desenvolver o termo $E[\mathcal{X}{\mathcal{X}^t}]$ da seguinte forma:

\begin{equation}\label{eqSeiSete}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})
\end{equation}

Lembrando o iníco da seção onde definimos as classes ${A_1},\,{A_2},\,...,\,{A_n}$ e a classe B onde podemos encontrar ${B_1},\,{B_2},\,...,\,{B_n}$, podemos encontrar as médias da equação \eqref{eqSeiSete} da seguinte forma:

\begin{equation}
\begin{array}{l}
E[\mathcal{X}{\mathcal{X}^t}] = E[\mathcal{X}{\mathcal{X}^t}|B]p(B) + \sum\limits_{j = 1}^n E [\mathcal{X}{\mathcal{X}^t}|{A_j}]p({A_j})\\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {p({A_j})\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} \\
 = p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t + } \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}} \sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} 
\end{array}
\end{equation}

Conforme verificamos na equação \eqref{eq_probabilidades}, obteremos a saída igual a 1 para o caso da variável aleatória X for classificada na classe $A_i$ e 0 caso contrário. Seguindo o raciocínio desenvolvido para a classe A temos as classes representadas através do complemento de $A_i$, dado por $A_i^C$:

\begin{equation}
\begin{array}{l}
E[\mathcal{XC}] = E[\mathcal{XC}|{A_i}]p({A_i}) + E[\mathcal{XC}|A_i^C](1 - p({A_i}))\\
 = E[\mathcal{X}|{A_i}]p({A_i}) + 0(1 - p({A_i}))\\
 = {p_i}\frac{1}{{{L_1}}}\sum\limits_{k = 1}^{{L_1}} {{A_{ik}}} 
\end{array}
\end{equation} 

Assim, a equação \eqref{vetorH}, pode ser reescrita da forma:

\begin{equation}
{h_{Ai}} = {\{ p(B)\frac{1}{r}\sum\limits_{j = 1}^r {{B_j}B_j^t}  + \sum\limits_{j = 1}^n {{p_j}\frac{1}{{{L_j}}}\sum\limits_{k = 1}^{{L_j}} {{A_{jk}}A_{jk}^t} } \} ^{ - 1}}\{ {p_i}\frac{1}{{{L_i}}}\sum\limits_{k = 1}^{{L_i}} {{A_{ik}}} \}
\end{equation}

Considerando-se a matriz de autocorrelação de $B$ igual a $R_B$ e a matriz $A_i$ igual a $R_{Ai}$, e a média $A_i$ igual a $\mu_{Ai}$, podemos expressar os resultados em função das variáveis aleatórias, assim, temos:

\begin{equation}\label{vetorH_final}
{h_{Ai}} = {\{ p(B){R_B} + \sum\limits_{j = 1}^n {{p_j}{R_{Aj}}} \} ^{ - 1}}\{\sum\limits_{i = 1}^m {{p_i}{\mu _{Ai}}}\}
\end{equation}

A condição de existência do classificador projetado é que o primeiro termo da equação \eqref{vetorH_final} seja uma matriz inversível e a dimensão dos vetores de entrada deve ser menor que a soma dos números de elementos de todas as classes $A_j$ e B, ou seja $d \le \,r\, + \,\sum\limits_{j = 1}^n {{L_j}}$.
              
\section{Detector por Produto Interno utilizando Análise de Componentes Principais}\label{cap-IPD-PCA}

Com o intuito de aumentar a robustez do detector por produto interno (IPD) a pequenas rotações, utilizou-se a técnica de análise de componentes principais (do inglês, \textit{principal component analysis} - PCA) \cite{waldir}. Essa técnica baseia-se na extração de um número desejado de componentes principais de um vetor multidimensional, de modo que ao final da operação tenhamos um vetor cuja dimensão é inferior à original e sua variância seja maximizada \cite{waldir}. A seguir comentaremos sobre o método denominado \textit{eigenpoint}, que é utilizado para encontrar pontos fiduciais\footnote{do inglês, \textit{fiducial point}, refere-se a características salientes comum as faces, como canto da boca, olho, nariz, etc.}

\subsection*{\textit{Eigenpoints}}

O termo \textit{eigenpoints} refere-se a um método que faz uso de PCA para a detecção de pontos fiduciais em imagens. A análise de componentes principais, é associada à ideia de redução de dimensionalidade,  com  menor  perda  possível  da  informação. Tendo em vista a similaridade dos pontos fiduciais humanos bem como a localização destes, utiliza-se essa técnica para representá-los em um subespaço de dimensão inferior ou igual ao original, assim, podemos utilizar \textit{eigenpoints} para encontrarmos os pontos fiduciais de um conjunto de faces, por exemplo.

 \textit{Eigenpoints} constitui-se um caso específico de \textit{eigenfaces} (Figura \ref{fig:eigenfacesEx}), porém, ao invés de utilizarmos toda a face tomamos um ponto fiducial de interesse e recortamos o bloco em que ele está contido, conforme pode ser visto na Figura \ref{fig:eigenpointsEx} (na Figura \ref{fig:eigenfacesEx} temos o \textit{eigenface} e na Figura \ref{fig:eigenpointsEx} recortamos o bloco no qual está contido o olho direito - \textit{eigenpoint}).

Matematicamente, podemos obter os \textit{eigenpoints} como sendo $M$ realizações de uma variável aleatória $\mathcal{X}_{N \times1}$ iguais a x$_{\textit{i}}$. Cada coluna x$_{\textit{i}}$ contém os \textit{pixels} de blocos centrados em pontos fiduciais ${\gamma_\textit{i}}$, onde \textit{i} = \{1,..., M\}. Dessa forma, \textbf{X} é uma matriz formada por \textit{M} vetores (sendo \textit{M} > \textit{N}) da seguinte forma:

\begin{equation}
\textbf{X} = [{\rm{x}_1} - {\mu _\mathcal{X}},\,...,\,{\rm{x}_M} - {\mu _\mathcal{X}}]
\end{equation}

onde $\mu _\mathcal{X}$ é:

\begin{equation}
{\mu _\mathcal{X}} = \frac{1}{M}\sum\limits_{i = 1}^M {{\rm{x}_\textit{i}}}
\end{equation}

Seja \textbf{$\mathbf{\Sigma}_\mathcal{X}$} a matriz de covariância de $\mathcal{X}$, dada por:

\begin{equation}
\mathbf{\Sigma}_\mathcal{X}  =  \frac{1}{{M - 1}}\textbf{X}{\textbf{X}^{*T}}
\end{equation}

Dessa forma, a base que contém as \textit{N} componentes principais descorrelacionadas (\textit{eingenpoints}) é encontrada através da diagonalização da matriz $\mathbf{\Sigma}_\mathcal{X}$, assim:

\begin{equation}
\mathbf{\Lambda} = \mathbf{\Phi}^{*t} \mathbf{\Sigma}_{\mathcal{X}} \mathbf{\Phi}.
\end{equation}

sendo $\Phi  = [{\phi _1},\,{\phi _2},\,...,\,{\phi _N}]$ é a matriz de autovetores de $\mathbf{\Sigma}_\mathcal{X}$, o sobrescrito \textit{T} indica o transposto da matriz de covariância e $\Lambda$ é a matriz diagonal contendo os valores de $\sum\nolimits_\mathcal{X}$, onde cada autovalor ${\lambda _\textit{i}}$ tem uma variância maior ou igual ao seu sucessor, ou seja, ${\lambda _1} \ge {\lambda _2} \ge ... \ge {\lambda _N}$. Dessa forma, é possível selecionar os componentes de maior energia através dos autovalores ${\lambda _i}$.

\begin{figure}[h]
\subfigure[Exemplos de \textit{eigenfaces}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenfacesEx.png}
\label{fig:eigenfacesEx}
}
\subfigure[Exemplos de \textit{eigenpoints}]{
\includegraphics[width=7.15cm,height=2cm]{figuras/eigenPointsEx.png}
\label{fig:eigenpointsEx}}
\caption{Exemplos de \textit{eigenfaces}  à direita e a esquerda temos \textit{eigenpoints} extraídos a partir de blocos centrados a partir do olho direito das imagens das faces.}
\end{figure}

\subsection*{Formulação do IPD-B-PCA}

O método utilizado consiste em aumentar a robustez do classificador IPD através das componentes principais de maior energia, ou seja, cujos autovalores associados são maiores \cite{waldir}. Matematicamente podemos obter esses filtros da seguinte maneira: suponhamos uma  variável aleatória que pode ser classificadas nas classes \{${\mathcal{A}_1},\,{\mathcal{A}_2},\,...,\,{\mathcal{A}_N}$\} onde: 

\begin{itemize}
\item A classe ${\mathcal{A}_1}$ será composta pela componente que desejamos detectar (${\mathcal{A}_1=\{\phi _i\}}$);

\item A classe ${\mathcal{A}_2}$ refere-se a deslocamentos lineares de $\phi _i$. A expressão $\phi _i^{n,m}$ relaciona o deslocamento linear da componente \textit{i} em \textit{n} \textit{pixels} na direção horizontal e \textit{m} na vertical. Considerando-se blocos quadrados de tamanho \textit{L} em que o centro do bloco contém o ponto fiducial de interesse, podemos escrever ${\mathcal{A}_2} = \{ \phi _i^{n,m}$, onde: $- L \le \,m,n\, \le \,L$, com $n,m \ne \,0\}$ .

\item A classe $\mathcal{B}$ será composta pelas demais componentes $\phi _j$, ou seja, as classes \newline\{${\mathcal{A}_3},\,{\mathcal{A}_4},\,...,\,{\mathcal{A}_N}$\}.
\end{itemize}

Para projetarmos o nosso classificador utilizaremos a equação \eqref{vetorH_final}, levando-se em consideração as premissas citadas teremos:

\begin{equation}\label{eq_ipd_pca}
{h_{{\phi _1}}} = {\left\{ {{p_1}{R_{A1}} + {p_2}{R_{A2}} + {p_B}{R_B}} \right\}^{ - 1}}\,{p_1}{R_{A1}}
\end{equation}

A Figura \ref{fig_ipd_pca} ilustra o esquema proposto, onde temos \textit{S} \textit{eigenpoints} ($\Phi  = \,{\phi _1},\,{\phi _2},\,...,\,{\phi _S}$), para cada um será projetado um filtro, de forma que estejam dispostos do maior para o menor valor de variância (autovalores), desse modo, ${\lambda _1} \ge \,{\lambda _2} \ge ,...,\,{\lambda _S}$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.9]{figuras/esquema_Matriz.png}
\caption{\textit{S} classificadores para \textit{S} componentes principais.}
\label{fig_ipd_pca}
\end{figure}

Na Figura \ref{fig_schema}, podemos observar um \textit{eigenpoint} que refere-se ao olho direito considerando-se apenas a primeira componente principal. A componente $\phi_1$ foi calculada através de blocos de dimensão $2L+1$ (imagem à esquerda). Afim de obtermos a componente $\phi_2$ através de deslocamentos lineares de meio bloco calculamos $\mathop {{\phi _i}}\limits^ \sim$ para blocos de dimensão $4L+1$ (imagem à direita). Assim, reescrevendo a equação \eqref{eq_ipd_pca} em função dessas ponderações teremos:

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{figuras/Schema.png}
\caption{Desclocamento linear}
\label{fig_schema}
\end{figure}


\begin{equation} \label{eq_IPD-B-PCA_Final}
h{\phi _i} = {\left\{ \begin{array}{l}
{p_1}{\phi _i}{({\phi _i})^{T}} + {p_2}\frac{1}{{{{(2L + 1)}^2} - 1}}\,\sum\limits_{n =  - L\hfill\atop
n \ne 0\hfill}^L {\sum\limits_{m =  - L\hfill\atop
m \ne 0\hfill}^L {{\phi _i}^{(n,m)}({\phi _i}^{(n,m)})^{T} }} +\\
 + p(\mathcal{B})\,\frac{1}{{N - 1}}\,\sum\limits_{j = 1\hfill\atop
j \ne i\hfill}^N {{\phi _j}{{({\phi _j})}^{T}}} 
\end{array} \right\}^{ - 1}}\,{p_1}{\phi _i}
\end{equation}

Finalmente, a equação \eqref{eq_IPD-B-PCA_Final} expressa o detector por produto interno utilizando análise de componentes principais. Salienta-se que na construção dos padrões de interesse da classe $\mathcal{A}_1$ podemos utilizar componentes levemente rotacionados ou deslocados, dessa forma é possível construir detectores mais robustos. A classe ${\mathcal{A}_2}$ é de fundamental importância, uma vez que se considerássemos somente as classes dos padrões de interesse poderíamos discriminar um falso positivo, ou seja, a classe $\mathcal{A}_1$ deslocada. Esse fato, deve-se a linearidade dos filtros, portanto, pequenas alterações na entrada produzem apenas pequenas perturbações na saída. Assim, a adição da classe ${\mathcal{A}_2}$ permite ao classificador rejeitar pequenos deslocamentos lineares de forma mais precisa e com uma maior eficiência.
                      
\section{Computação Paralela}

A ideia central da computação paralela é de que um tarefa geralmente pode ser dividida em partes menores e executadas de forma concorrente. Atualmente existem duas abordagens no que tange a computação paralela, a primeira diz respeito aos \textit{multi-cores}, integra alguns núcleos (entre dois e dez) em um único microprocessador, como exemplo temos \textit{desktops}, \textit{smartphones}, etc. A segunda relaciona-se aos \textit{many-cores} e faz uso de um grande número de núcleos (geralmente várias centenas) e está especialmente orientada à execução de programas paralelos, o principal exemplo desse modelo são as GPUs (\textit{Graphics Processing Unit}) \cite{diaz2012survey}.

Nas literaturas uma plataforma geralmente é definida como a combinação do \textit{hardware} com um sistema operacional o qual coordena o conjunto de instruções de um processador. Com o advento da tecnologia \textit{multi-core} a forma de processamento mudou, dividindo-se a carga de trabalho entre os processadores existentes. Quando a paralelização dessas tarefas é feita pelo próprio sistema operacional dizemos que se trata de um paralelismo implícito, em contrapartida quando a paralelização é feita pelo usuário (programador) dizemos que a paralelização é explícita.

Entende-se por modelo de programação a interface que estabelece uma ligação entre a linguagem de programação e a arquitetura do sistema. Existem duas arquiteturas básicas que divergem quanto ao acesso a memória, que podem ser compartilhado (onde existe uma única memória que é compartilhada para todos os núcleos) e distribuída (quando cada núcleo tem sua própria memória). Diversos outros modelos podem ser encontrados na literatura como os baseados em troca de mensagens, \textit{threads}, híbridos, entre outros. Existem diversas ferramentas que facilitam a implementação da paralelização em um algoritmo, falaremos sobre as principais a seguir.

O MPI (\textit{Message Passing}), é uma biblioteca padrão de programação paralela aplicado a memórias distribuídas, que faz uso de troca de mensagens para a comunicação inter-processos. Dessa forma, os programas podem ser compilados em linguagens como C ou C++ e ligados a essa biblioteca. O objetivo do MPI é estabelecer um padrão portável, eficiente e flexível na execução de  programas que utilizem troca de mensagens, além de garantir a portabilidade dos nos diversos tipos de máquinas paralelas que fazem seu uso. Em MPI o particionamento de carga de trabalho e mapeamento de tarefas deve ser feito pelo programador, assim, a sincronização nesse modelo é um fator de risco. Esse modelo é comumente utilizado em \textit{clusters} de \textit{workstations}

POSIX \textit{Threads} (do inglês, \textit{Portable Operating System Interface} - Interface Portável entre Sistemas Operacionais), também conhecido como \textit{Pthreads} é um modelo baseado em memória compartilhada que padroniza o uso de \textit{threads} entre diferentes sistemas operativo/arquiteturas. Esse modelo foi especificado pelo padrão IEEE 1003.1c POSIX, de modo que implementações que aderem a este padrão são referidos como POSIX \textit{threads} , ou Pthreads, e implementam uma API para a escrita de aplicações \textit{multithreads}. A biblioteca Pthreads fornece funções para criar e destruir \textit{threads} e para coordenar suas atividades através de acesso exclusivo em locais selecionados de memória. Este modelo é especialmente adequado para o padrão de junção \textit{fork}/\textit{join} de programação paralela.

O OpenMP é uma API cujo o objetivo é o desenvolvimento de aplicações \textit{multithreads} de uma forma mais fácil em ambientes de programação C/C++ e Fortran. O OpenMP utiliza-se de memória compartilhada, desse modo, cada \textit{core} poderá ler e escrever em todo o espaço da memória do sistema. Esse modelo possibilita o paralelismo incremental além de combinações de parte de código escrito de forma serial e parte paralela, em um único código fonte. Nesse modelo nem todas as características são explícitas como no MPI, a sincronização das tarefas que serão executadas de forma concorrente serão controladas pelo programador enquanto o mapeamento das tarefas entre os processadores não. A API trabalha em um nível de abstração mais elevado do que as \textit{threads}, é portável entre arquiteturas de memória compartilhada, e a transição dos trechos sequenciais-paralelos se dão através da estrutura \textit{fork/join}. O elevado nível de abstração e a facilidade da API tornam o OpenMP adequado para o desenvolvimento de aplicações em sistemas de memória compartilhada \cite{diaz2012survey}.

Para sistemas híbridos, isso é, aqueles que dispões de memórias compartilhada e distribuída, é necessário coexistir, no mínimo, dois modelos. A integração de dois ou mais modelos de modo a permitir utilizar todo o poder computacional é um desafio. Na tabela \ref{tab_difPar}, fazemos um resumo dos principais modelos descritos bem como suas principais características, perceba que alguns modelos podem ser implementados tanto em memória compartilhada como em distribuída.

Estudos a respeito do uso de núcleos de processamento (\textit{cores}) em telefones celulares com o Sistema Operacional Android demonstram que estes são usados de maneira ineficiente \cite{gao2015study} \cite{wu2013study}. Isso ocorre principalmente pela maneira de programação dos \textit{softwares}, a maioria não explora recursos de  paralelização de maneira eficiente, outro problema observado tange a paralelização implícita não ser tão eficaz devido a complexidade de um algoritmo transformar trechos de códigos sequenciais em paralelos. Inerentemente algumas partes do código são sequenciais (inicialização, instanciação de tarefas, por exemplo) e isso provoca um aumento no tempo de execução, porém os trechos que podem ser paralelizáveis anda são pouco explorados \cite{gao2015study}.

\begin{center}
\begin{table}[tbh]
\centering
\newcolumntype{M}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand\arraystretch{1} \setlength\minrowclearance{2.4pt}
\caption{Modelos de Programação Paralela Puros}
\vspace{0.5cm}
\label{tab_difPar}
\begin{tabular}{|l|M{2cm}|M{3cm}|M{3cm}|}
\hline
\textbf{Implementação} & \textbf{Pthreads} & \textbf{OpenMP} & \textbf{MPI} \\ 
\hline
\textbf{Modelo de Programação} & \textit{Threads} & Memória Compartilhada & Troca de mensagens \\ 
\hline
\textbf{Arquitetura do Sistema} & Memória Compartilhada & Memória Compartilhada & Memória Distribuída e Compartilhada \\ 
\hline
\textbf{Modelo de comunicação} & Endereçamento compartilhado & Endereçamento compartilhado & Troca de mensagens ou Endereçamento compartilhado \\ 
\hline
\textbf{Sincronização} & Explícita & Implícita & Implícita ou Explícita \\ 
\hline
\textbf{Implementação} & Biblioteca & Compilador & Biblioteca \\ 
\hline
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ 
\end{tabular}
\end{table}
\end{center}

\section{Sistema IPD-PCA}

Para a implementação do sistema proposto levamos em consideração que o problema de aprendizagem é supervisionado, logo, temos que os padrões utilizados para o treinamento possuem uma classe pré-especificada. O sistema será dividido em duas etapas: treino e teste.

A etapa de treino será dividida da seguinte forma: Procedimentos iniciais de treinamento, projeto dos detectores $\rm h{\phi _i}$, em seguida é realizado o uma operação de produto interno e por último é realizado o treinamento do classificador. A etapa de teste é muito semelhante a de treino, ela será dividida da seguinte forma: procedimentos iniciais para treinamento, operação de produto interno com os detectores $\rm h{\phi _i}$ (obtidos no treinamento), classificação e pós-processamento. As fases de Procedimentos Iniciais para Treinamento são idênticas em ambas as fases, o esquema da Figura \ref{fig_System_Schema} ilustra o sistema, a seguir será detalhado cada uma das etapas.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{figuras/System_Diagram.png}
\caption{Diagrama em bloco so sistema.}
\label{fig_System_Schema}
\end{figure}

\subsection{Treino}\label{treino}

Nessa fase desenvolvemos o aprendizado de todos os modelos dos detectores IPD, para isso usaremos uma base de dados contendo faces humanas frontais em nível de cinza, as faces possuem pequenas variações de escala (perto e mais afastado da câmera), iluminação e pequenas rotações. A Figura \ref{fig_Train_Schema} ilustra o diagrama em blocos da etapa de treinamento, a seguir explanaremos sobre o sistema.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{figuras/Schema_Train.png}
\caption{Diagrama em bloco da etapa de treinamento.}
\label{fig_Train_Schema}
\end{figure}


\subsubsection{Procedimentos iniciais para o treinamento}\label{Pre-Proc}

A seguir detalharemos o primeiro bloco da Figura \ref{fig_Train_Schema}. Esse bloco é comum tanto a etapas de treinamento e teste, e conforme ilustrado na Figura \ref{fig_Pre-Proc}, é composto das seguintes passos: enquadramento do rosto através do método de \textit{Viola-Jones}, escalonamento da imagem, correção de iluminação, modelo gaussiano à priori (redução do espaço de busca) e definição das classes positivos e negativos. Sequencialmente cada uma das imagens passarão pelos seguintes procedimentos de modo a extrair as características desejadas para a construção de um modelo:

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.55]{figuras/PIT_Pre_Proc.png}
\caption{Diagrama em bloco da fase Procedimentos Iniciais de treinamento (adaptado de Sabino\cite{waldir}).}
\label{fig_Pre-Proc}
\end{figure}

\begin{itemize}
\item \textbf{Detector de faces VJ}: É realizado o enquadramento de face na imagem de entrada através do algoritmo de Viola-Jones \cite{viola2004robust};

\item \textbf{Escalonamento}: Em seguida a imagem é reescalada. Essa técnica é utilizada pois as diferentes imagens não tem a mesma distância da câmera;

\item \textbf{Correção de Iluminação}: A seguir é aplicada a uma correção de iluminação através da correção do fator gama, filtragem por diferença de gaussianas e equalização de contraste \cite{tan2010enhanced};

\item \textbf{Modelo Gaussiano à Priori}: Feita a correção de iluminação é definido uma redução no espaço de busca segundo um modelo probabilístico gaussiano;

\item \textbf{Definição das classes Positivos e Nagativos}: Finalmente serão definidas como positivos e negativos conforme explicado na seção \ref{cap-IPD-PCA}, onde as classes $\mathcal{A}_1$ será definida como positivo
por se tratar de blocos com centros em pontos fiduciais e as classes $\mathcal{A}_2$ e $\mathcal{B}$ serão definidas como negativos por tratarem de outros blocos.
\end{itemize}


\subsubsection{Procedimentos para Treinamento}\label{Procedimentos-Treino}

A seguir será explicado o funcionamento do restante dos blocos da Figura \ref{fig_Train_Schema}. Passado o bloco de procedimentos iniciais, serão projetados \textit{k} detectores $\rm \textbf{h}_{\phi _i}$ (\textit{i}={1,...,\textit{k}}), um para cada uma das componentes principais com maiores variâncias utilizando-se a Equação \ref{eq_IPD-B-PCA_Final}. Depois de encontrados os \textit{k} vetores é iniciado o treinamento do classificador AdaBoost\footnote{\textit{Adaptative Boosting}, é a técnica de aprendizado na qual podemos construir classificadores fortes a partir de uma combinação de classicadores fracos.}, que será responsável pela detecção das classes. Cada bloco pertencente as classes de positivos e negativos (saída do bloco de procedimentos iniciais para Treinamento) será subtraído de um bloco médio de todas as classes de positivos (denominado $\mu_{U}$), em seguida, serão processados utilizando-se o produto interno $\rm h_{\phi i}^{*T}$${B_Z}$. Desse modo, cada bloco ${B_Z}$ terá um vetor associado denominado $\rm d$$_{B_Z}$ com dimensões 1x\textit{k}. O conjunto de vetores $\rm d$$_{B_Z}$ serão utilizados no treinamento do classificador  de modo que possamos discriminar blocos positivos e negativos na outra etapa (teste).

\subsection{Teste}

A Figura \ref{fig_Schema_Test} ilustra o diagrama em blocos que representa a fase de teste para o sistema IPD-PCA, que será detalhado a seguir.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.78]{figuras/Schema_Test.png}
\caption{Treinamento dos classificadores $\rm 
h_{\phi _i}$ (adaptado de Sabino\cite{waldir}).} 
\label{fig_Schema_Test}
\end{figure}

Nessa etapa verifica-se o desempenho do sistema de detecção de pontos fiduciais, a descrição dessa etapa é feita a seguir. Inicialmente realizamos os Procedimentos Iniciais para Teste, utilizando o mesmo procedimento descrito na Seção \ref{Pre-Proc}, posteriormente utilizando uma janela deslizante selecionamos uma quantidade de blocos $B_{Z}$ aleatória em cada região de interesse por imagem (onde z é a coordenada correspondente ao centro do bloco). Em seguida processamos o produto interno dos detectores $\rm h_{\phi _i}$ (obtidos na etapa de treinamento) com $B_{Z}$ subtraído por $\mu_{U}$ (bloco médio de todas as classes de positivos, determinado na etapa de treino). Assim, cada bloco $B_{Z}$ terá um vetor $\rm d$$_{B_Z}$ associado, esse vetor  será usado para classificar $B_{Z}$. Em seguida é feito um pós-processamento conforme a estratégia escolhida.

A saída do classificador AdaBoost é uma nuvem de pontos, conforme pode ser visto da Figura \ref{Pre-Proc}. A partir dessa nuvem escolheremos apenas uma coordenada que será candidato eleito a ponto fiducial. Com base em \cite{waldir}, temos quatro estratégias discutidas a seguir:

\begin{itemize}
\item A estratégia de acrônimo \textit{NA} não realiza nenhuma modificação na saída do bloco AdaBoost. Assim, todos os elementos da nuvem são considerados saídas do método;

\item A segunda estratégia é denominada \textit{ML}, ela seleciona a coordenada mais provável, supondo o modelo gaussiano a priori como critério de decisão;

\item A estratégia \textit{GML} agrupa os elementos com distância menor que \textit{P} pixels e seleciona os resultados dos agrupamentos conforme \textit{ML};

\item A estratégia \textit{A} dá como saída do método a média dos elementos da nuvem;
\end{itemize}

\chapter{Proposta}\label{cap-especifico}

\section{Contextualização do Problema} \label{sec_Problem}

Diversos estudos acerca de desempenho computacional tem-se desenvolvido, uma dessas linhas de pesquisa diz respeito a utilização de \textit{cores} (ou núcleos de processamento). Trabalhos como de \cite{flautner2000thread}, \cite{gao2015study}, \cite{wu2013study} apontam que os \textit{cores}, de modo geral, são subutilizados e o incremento na quantidade de núcleos não traduz de forma proporcional em ganho de performance. As aplicações anteriores aos processadores \textit{multi-cores} utilizavam-se do paradigma sequencial de programação, e muitas aplicações atuais não exploram recursos de paralelização das tarefas \cite{flautner2000thread}.

Os sistemas embarcados por sua vez também acompanharam a evolução dos processadores e incorporaram a arquitetura \textit{multi-core}. Estudos realizados por \cite{gao2015study} apontam, por exemplo, que a plataforma Android não explora de maneira eficiente todos os recursos do seu processador, fazendo assim com que alguns processos levem mais tempo para serem executados além de sobrecarregar outros \textit{cores}. Salienta-se que isso ocorre, geralmente, por causa do paralelismo implícito (no qual o próprio sistema operativo é encarregado de paralelizar trechos das aplicações), de modo que a paralelização de um programa é tarefa extremamente complexa e variável para um algoritmo do sistema operacional fazer sempre de forma eficiente.

Embora o processamento dos sistemas embarcados estejam cada dia mais velozes, de um modo geral, o processamento digital de imagens ainda demanda por bastante poder de processamento. Se tomarmos como referência aplicações Android, perceberemos que a maioria não explora de maneira satisfatória todos os recursos dos seus núcleos, nem mesmo as aplicações nativas (desenvolvidos pelo próprio Google), conforme afirma \cite{gao2015study}. Esse fato aliado a pouca quantidade de memória RAM que possui a maioria desses dispositivos nos permite inferir que há uma sobrecarga de processamento em aplicações mais pesadas como, por exemplo, as que detectam pontos fiduciais e/ou reconhecimento facial.

A ideia central desse trabalho é de identificar as estruturas paralelizáveis no modelo IPD-PCA proposto por \cite{waldir} em um \textit{smartphone} de plataforma Android e processá-las usando técnicas de computação paralela. Nas seções posteriores serão detalhados as abordagens e modificações no modelo.


\section{Modelo Proposto} \label{sec_ModeloProposto}

As Figuras \ref{fig:ParalelismoTreino} e \ref{fig:ParalelismoTeste} ilustram as modificações na etapa de treino e de teste, respectivamente, propostas nesse trabalho. Conforme podemos perceber a fase de Procedimentos Iniciais para Treinamento é a mesma para ambas as etapas (Treino e Teste), ela constitui-se dos procedimentos mostrados na seção \ref{Pre-Proc} de forma sequencial, desse modo não há meios de paralelizá-lo.

A estrutura Projeto de $\rm h_{\phi _i}$ (Figura \ref{fig:ParalelismoTreino} - bloco cinza), por sua vez é uma das partes a qual se propõe paralelizar. Para cada ponto fiducial, um conjunto de detectores em cascata é utilizado para realizar a detecção, sendo que esse conjunto será constituído \textit{N} detectores (um detector para cada uma das componentes principais) conforme definido na seção \ref{sec_IPD}, o número de componentes principais na maioria dos pontos fiduciais definidos por \cite{waldir2010facial} foi de \textit{N}=63, com \textit{N}=53 nos demais pontos. A intenção é de que cada um desses detectores possam ser construídos de forma paralela modificando assim a característica sequencial do algoritmo.

Outra estrutura no qual é proposta mudança diz respeito ao Produto Interno $\rm h_{\phi i}^{*T}$${B_Z}$ (Figura \ref{fig:ParalelismoTreino} e Figura \ref{fig:ParalelismoTeste} - bloco cinza). Nessa fase cada um dos \textit{N} detectores realizará uma operação de produto interno com cada um dos blocos $B_{Z}$ definidos na seção \ref{Pre-Proc}. Tomando como referência \cite{waldir} que definiu 50 blocos negativos e 1 positivo para cada imagem e adotando 53 detectores teremos 2703 operações de produto interno por imagem, se levarmos em consideração que o mesmo utilizou 503 imagens teremos um total de 1359609 operações de produto interno. A proposta de paralelizar essas operações pressupõe ganho em eficiência, uma vez que deixará de ser realizada de forma sequencial.

\begin{figure}[h]
\subfigure[Esquema da paralelização proposta para o bloco de Treino]{
\includegraphics[scale=0.64]{figuras/Parallelism_Block_Train.png}
\label{fig:ParalelismoTreino}
}
\subfigure[Esquema da paralelização proposta para o bloco de Teste]{
\includegraphics[scale=0.72]{figuras/Parallelism_Block_Test.png}
\label{fig:ParalelismoTeste}}
\caption{Esquema de modificações proposto para as etapas de Treino e de Teste}
\end{figure}

A principal contribuição do trabalho é paralelizar o algoritmo do detector IPD, uma alternativa para isso seria a utilização da API OpenMP em conjunto com a biblioteca OpenCV (\textit{Open Source Computer Vision Library}) de modo a facilitar o trabalho. O OpenMP vem de encontro com a proposta do trabalho pois trata-se de um sistema com memória compartilhada e de código fonte aberto, além de oferecer relativa facilidade devido ao seu modelo de abstração ser mais alto.

\section{Avaliações de Desempenho} \label{sec_Desempenho}

Nem todas as aplicações obtém a mesma melhora no desempenho quando executadas com paralelismo. Isso depende da quantidade de código paralelizável que a aplicação possui. Algumas métricas são comumente utilizadas para medir o desempenho de sistemas paralelos. São elas, o tempo de execução, \textit{speedup}, eficiência e o custo \cite{de2008aspectos}.

O \textbf{tempo de execução} de um algoritmo é o tempo necessário para resolver um problema. O tempo de processamento de um algoritmo é o tempo gasto executando o processamento sem contar a comunicação e a ociosidade. 

Em computação paralela, o \textit{\textbf{speedup}} é utilizado para conhecer o quão um algoritmo paralelo é mais rápido que seu correspondente sequencial. Quanto mais trechos forem paralelizáveis menor será o tempo gasto, o \textit{Speedup} pode ser obtido pela equação \ref{eq_speedup}.

\begin{equation} \label{eq_speedup}
Speedup = \frac{{Tempo\,Serial}}{{Tempo\,Paralelo}}
\end{equation}

Outra métrica de desempenho para se medir o ganho em ambientes paralelos é a \textbf{eficiência} que é determinada pela fração entre o \textit{Speedup} de \textit{p} processadores sobre \textit{p}, conforme pode ser visto na equação \ref{eq_eficiencia}.

\begin{equation} \label{eq_eficiencia}
efficienc{y_{(p)}} = \frac{{Speedu{p_{(p)}}}}{p}
\end{equation}

O \textbf{custo} reflete a soma do tempo que cada processador  gasta resolvendo um problema, essa métrica é o produto do tempo de execução  e o número de processadores utilizados. Para um único processador, o custo é o tempo de execução do algoritmo sequencial mais rápido conhecido.